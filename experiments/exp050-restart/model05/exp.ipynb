{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from src.dataloader_ import *\n",
    "# from src.network_ import *\n",
    "from src.utils import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "\n",
    "train_series_dir = \"../../../inputs/series_train.parquet/\"\n",
    "test_series_dir = \"../../../inputs/series_test.parquet/\"\n",
    "\n",
    "data_dic_path = \"../../../inputs/data_dictionary.csv\"\n",
    "sample_submission_path = \"../../../inputs/sample_submission.csv\"\n",
    "train_path = \"../../../inputs/train.csv\"\n",
    "test_path = \"../../../inputs/test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "data_dic = pd.read_csv(data_dic_path)\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "nb_name = os.path.basename(os.getcwd())  # notebook name\n",
    "seed_torch(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの前処理\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "add_features = [\n",
    "    \"BMI_Age\",\n",
    "    \"Internet_Hours_Age\",\n",
    "    \"BMI_Internet_Hours\",\n",
    "    \"BFP_BMI\",\n",
    "    \"FFMI_BFP\",\n",
    "    \"FMI_BFP\",\n",
    "    \"LST_TBW\",\n",
    "    \"BFP_BMR\",\n",
    "    \"BFP_DEE\",\n",
    "    \"BMR_Weight\",\n",
    "    \"DEE_Weight\",\n",
    "    \"SMM_Height\",\n",
    "    \"Muscle_to_Fat\",\n",
    "    \"Hydration_Status\",\n",
    "    \"ICW_TBW\",\n",
    "]\n",
    "\n",
    "double_columns = [\n",
    "    \"FGC-FGC_SRR_Zone\",\n",
    "    \"BIA-BIA_SMM\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"BIA-BIA_FFMI\",\n",
    "    \"FGC-FGC_CU\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"BIA-BIA_ECW\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "    \"FGC-FGC_SRL_Zone\",\n",
    "    \"BIA-BIA_DEE\",\n",
    "    \"Physical-Weight\",\n",
    "    \"Fitness_Endurance-Time_Mins\",\n",
    "    \"FGC-FGC_SRR\",\n",
    "    \"SDS-SDS_Total_T\",\n",
    "    \"FGC-FGC_PU\",\n",
    "    \"BIA-BIA_FFM\",\n",
    "    \"FGC-FGC_TL_Zone\",\n",
    "    \"Physical-BMI\",\n",
    "    \"Physical-Systolic_BP\",\n",
    "    \"Physical-HeartRate\",\n",
    "    \"BIA-BIA_ICW\",\n",
    "    \"Physical-Height\",\n",
    "    \"FGC-FGC_SRL\",\n",
    "    \"BIA-BIA_BMC\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"BIA-BIA_Frame_num\",\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"FGC-FGC_GSND_Zone\",\n",
    "    \"Basic_Demos-Sex\",\n",
    "    \"FGC-FGC_GSND\",\n",
    "    \"BIA-BIA_LST\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"BIA-BIA_BMI\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"PAQ_C-PAQ_C_Total\",\n",
    "    \"BIA-BIA_Activity_Level_num\",\n",
    "    \"FGC-FGC_GSD\",\n",
    "    \"BIA-BIA_BMR\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"CGAS-CGAS_Score\",\n",
    "    \"FGC-FGC_PU_Zone\",\n",
    "    \"BIA-BIA_LDM\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"PAQ_A-PAQ_A_Total\",\n",
    "    \"BIA-BIA_TBW\",\n",
    "    \"FGC-FGC_GSD_Zone\",\n",
    "    \"Physical-Diastolic_BP\",\n",
    "]\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # season_cols = [col for col in df.columns if \"Season\" in col]\n",
    "    # df = df.drop(season_cols, axis=1)\n",
    "    df[\"BMI_Age\"] = df[\"Physical-BMI\"] * df[\"Basic_Demos-Age\"]\n",
    "    df[\"Internet_Hours_Age\"] = (\n",
    "        df[\"PreInt_EduHx-computerinternet_hoursday\"] * df[\"Basic_Demos-Age\"]\n",
    "    )\n",
    "    df[\"BMI_Internet_Hours\"] = (\n",
    "        df[\"Physical-BMI\"] * df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "    )\n",
    "    df[\"BFP_BMI\"] = df[\"BIA-BIA_Fat\"] / df[\"BIA-BIA_BMI\"]\n",
    "    df[\"FFMI_BFP\"] = df[\"BIA-BIA_FFMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"FMI_BFP\"] = df[\"BIA-BIA_FMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"LST_TBW\"] = df[\"BIA-BIA_LST\"] / df[\"BIA-BIA_TBW\"]\n",
    "    df[\"BFP_BMR\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_BMR\"]\n",
    "    df[\"BFP_DEE\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_DEE\"]\n",
    "    df[\"BMR_Weight\"] = df[\"BIA-BIA_BMR\"] / df[\"Physical-Weight\"]\n",
    "    df[\"DEE_Weight\"] = df[\"BIA-BIA_DEE\"] / df[\"Physical-Weight\"]\n",
    "    df[\"SMM_Height\"] = df[\"BIA-BIA_SMM\"] / df[\"Physical-Height\"]\n",
    "    df[\"Muscle_to_Fat\"] = df[\"BIA-BIA_SMM\"] / df[\"BIA-BIA_FMI\"]\n",
    "    df[\"Hydration_Status\"] = df[\"BIA-BIA_TBW\"] / df[\"Physical-Weight\"]\n",
    "    df[\"ICW_TBW\"] = df[\"BIA-BIA_ICW\"] / df[\"BIA-BIA_TBW\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "for add_ in add_features:\n",
    "    train[add_] = train[add_].fillna(0.0)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "\n",
    "test = feature_engineering(test)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "for add_ in add_features:\n",
    "    test[add_] = test[add_].fillna(0.0)\n",
    "test = test.dropna(thresh=10, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Tabnet***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "import os\n",
    "import torch\n",
    "from pytorch_tabnet.callbacks import Callback\n",
    "\n",
    "\n",
    "class TabNetWrapper(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.model = TabNetRegressor(**kwargs)\n",
    "        self.kwargs = kwargs\n",
    "        self.imputer = SimpleImputer(strategy=\"median\")\n",
    "        self.best_model_path = \"best_tabnet_model.pt\"\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Handle missing values\n",
    "        X_imputed = self.imputer.fit_transform(X)\n",
    "\n",
    "        if hasattr(y, \"values\"):\n",
    "            y = y.values\n",
    "\n",
    "        # Create internal validation set\n",
    "        X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "            X_imputed, y, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        # Train TabNet model\n",
    "        history = self.model.fit(\n",
    "            X_train=X_train,\n",
    "            y_train=y_train.reshape(-1, 1),\n",
    "            eval_set=[(X_valid, y_valid.reshape(-1, 1))],\n",
    "            eval_name=[\"valid\"],\n",
    "            eval_metric=[\"mse\"],\n",
    "            max_epochs=200,\n",
    "            patience=20,\n",
    "            batch_size=1024,\n",
    "            virtual_batch_size=128,\n",
    "            num_workers=0,\n",
    "            drop_last=False,\n",
    "            callbacks=[\n",
    "                TabNetPretrainedModelCheckpoint(\n",
    "                    filepath=self.best_model_path,\n",
    "                    monitor=\"valid_mse\",\n",
    "                    mode=\"min\",\n",
    "                    save_best_only=True,\n",
    "                    verbose=True,\n",
    "                )\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        # Load the best model\n",
    "        if os.path.exists(self.best_model_path):\n",
    "            self.model.load_model(self.best_model_path)\n",
    "            os.remove(self.best_model_path)  # Remove temporary file\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_imputed = self.imputer.transform(X)\n",
    "        return self.model.predict(X_imputed).flatten()\n",
    "\n",
    "    def __deepcopy__(self, memo):\n",
    "        # Add deepcopy support for scikit-learn\n",
    "        cls = self.__class__\n",
    "        result = cls.__new__(cls)\n",
    "        memo[id(self)] = result\n",
    "        for k, v in self.__dict__.items():\n",
    "            setattr(result, k, deepcopy(v, memo))\n",
    "        return result\n",
    "\n",
    "\n",
    "# TabNet hyperparameters\n",
    "TabNet_Params = {\n",
    "    \"n_d\": 64,  # Width of the decision prediction layer\n",
    "    \"n_a\": 64,  # Width of the attention embedding for each step\n",
    "    \"n_steps\": 5,  # Number of steps in the architecture\n",
    "    \"gamma\": 1.5,  # Coefficient for feature selection regularization\n",
    "    \"n_independent\": 2,  # Number of independent GLU layer in each GLU block\n",
    "    \"n_shared\": 2,  # Number of shared GLU layer in each GLU block\n",
    "    \"lambda_sparse\": 1e-4,  # Sparsity regularization\n",
    "    \"optimizer_fn\": torch.optim.Adam,\n",
    "    \"optimizer_params\": dict(lr=2e-2, weight_decay=1e-5),\n",
    "    \"mask_type\": \"entmax\",\n",
    "    \"scheduler_params\": dict(mode=\"min\", patience=10, min_lr=1e-5, factor=0.5),\n",
    "    \"scheduler_fn\": torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    \"verbose\": 1,\n",
    "    \"device_name\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "\n",
    "class TabNetPretrainedModelCheckpoint(Callback):\n",
    "    def __init__(\n",
    "        self, filepath, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1\n",
    "    ):\n",
    "        super().__init__()  # Initialize parent class\n",
    "        self.filepath = filepath\n",
    "        self.monitor = monitor\n",
    "        self.mode = mode\n",
    "        self.save_best_only = save_best_only\n",
    "        self.verbose = verbose\n",
    "        self.best = float(\"inf\") if mode == \"min\" else -float(\"inf\")\n",
    "\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.model = self.trainer  # Use trainer itself as model\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        current = logs.get(self.monitor)\n",
    "        if current is None:\n",
    "            return\n",
    "\n",
    "        # Check if current metric is better than best\n",
    "        if (self.mode == \"min\" and current < self.best) or (\n",
    "            self.mode == \"max\" and current > self.best\n",
    "        ):\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f\"\\nEpoch {epoch}: {self.monitor} improved from {self.best:.4f} to {current:.4f}\"\n",
    "                )\n",
    "            self.best = current\n",
    "            if self.save_best_only:\n",
    "                self.model.save_model(self.filepath)  # Save the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2188, 63) (548, 63)\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002532 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 9439\n",
      "[LightGBM] [Info] Number of data points in the train set: 2188, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 0.579068\n",
      "Fold 1 - Train QWK: 0.9164, Validation QWK: 0.3351\n",
      "(2189, 63) (547, 63)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9428\n",
      "[LightGBM] [Info] Number of data points in the train set: 2189, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 0.581087\n",
      "Fold 2 - Train QWK: 0.9156, Validation QWK: 0.3863\n",
      "(2189, 63) (547, 63)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001738 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9438\n",
      "[LightGBM] [Info] Number of data points in the train set: 2189, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 0.580630\n",
      "Fold 3 - Train QWK: 0.9079, Validation QWK: 0.3747\n",
      "(2189, 63) (547, 63)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000628 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9438\n",
      "[LightGBM] [Info] Number of data points in the train set: 2189, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 0.580630\n",
      "Fold 4 - Train QWK: 0.9097, Validation QWK: 0.4105\n",
      "(2189, 63) (547, 63)\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001084 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9377\n",
      "[LightGBM] [Info] Number of data points in the train set: 2189, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 0.580630\n",
      "Fold 5 - Train QWK: 0.9162, Validation QWK: 0.3833\n",
      "CV: 0.3780\n",
      "tuned Kappa: 0.439\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (\n",
    "    VotingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "\n",
    "\n",
    "def extract_features(df):\n",
    "    return df[double_columns + add_features]\n",
    "\n",
    "\n",
    "seed = 42\n",
    "oof = []\n",
    "cv_scores = []\n",
    "y = None\n",
    "\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    \"learning_rate\": 0.046,\n",
    "    \"max_depth\": 12,\n",
    "    \"num_leaves\": 478,\n",
    "    \"min_data_in_leaf\": 13,\n",
    "    \"feature_fraction\": 0.893,\n",
    "    \"bagging_fraction\": 0.784,\n",
    "    \"bagging_freq\": 4,\n",
    "    \"lambda_l1\": 10,  # Increased from 6.59\n",
    "    \"lambda_l2\": 0.01,  # Increased from 2.68e-06\n",
    "}\n",
    "XGB_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"n_estimators\": 200,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 1,  # Increased from 0.1\n",
    "    \"reg_lambda\": 5,  # Increased from 1\n",
    "    \"random_state\": seed,\n",
    "}\n",
    "\n",
    "CatBoost_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 6,\n",
    "    \"iterations\": 200,\n",
    "    \"random_seed\": seed,\n",
    "    \"verbose\": 0,\n",
    "    \"l2_leaf_reg\": 10,  # Increase this value\n",
    "}\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "for fold in range(5):\n",
    "    Light = LGBMRegressor(**Params, random_state=seed, verbose=-1, n_estimators=300)\n",
    "    XGB_Model = XGBRegressor(**XGB_Params)\n",
    "    CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "    TabNet_Model = TabNetWrapper(**TabNet_Params)  # New\n",
    "\n",
    "    SEED = 42\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    model = VotingRegressor(\n",
    "        estimators=[\n",
    "            (\n",
    "                \"lgb\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\"imputer\", imputer),\n",
    "                        (\"regressor\", LGBMRegressor(random_state=SEED)),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"xgb\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\"imputer\", imputer),\n",
    "                        (\"regressor\", XGBRegressor(random_state=SEED)),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"cat\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\"imputer\", imputer),\n",
    "                        (\n",
    "                            \"regressor\",\n",
    "                            CatBoostRegressor(random_state=SEED, silent=True),\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"rf\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\"imputer\", imputer),\n",
    "                        (\"regressor\", RandomForestRegressor(random_state=SEED)),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                \"gb\",\n",
    "                Pipeline(\n",
    "                    steps=[\n",
    "                        (\"imputer\", imputer),\n",
    "                        (\"regressor\", GradientBoostingRegressor(random_state=SEED)),\n",
    "                    ]\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # model = LGBMRegressor(**Params, random_state=seed, verbose=-1, n_estimators=300)\n",
    "\n",
    "    with open(f\"../divided-datasets/fold_train_ids_{fold}.pkl\", \"rb\") as f:\n",
    "        fold_train_ids = pickle.load(f)\n",
    "\n",
    "    with open(f\"../divided-datasets/fold_valid_ids_{fold}.pkl\", \"rb\") as f:\n",
    "        fold_valid_ids = pickle.load(f)\n",
    "\n",
    "    train_fold = train[train[\"id\"].isin(fold_train_ids)].reset_index(drop=True)\n",
    "    valid_fold = train[train[\"id\"].isin(fold_valid_ids)].reset_index(drop=True)\n",
    "\n",
    "    mode = \"drop\"\n",
    "\n",
    "    if mode == \"impute\":\n",
    "        numeric_cols = train.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        train_fold[numeric_cols] = imputer.fit_transform(train_fold[numeric_cols])\n",
    "        test[numeric_cols] = imputer.transform(test[numeric_cols])\n",
    "    elif mode == \"drop\":\n",
    "        train_fold = train_fold[train_fold[\"sii\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "    train_fold_x = extract_features(train_fold)\n",
    "    valid_fold_x = extract_features(valid_fold)\n",
    "\n",
    "    train_fold_y = train_fold[\"sii\"].astype(int)\n",
    "    valid_fold_y = valid_fold[\"sii\"].astype(int)\n",
    "\n",
    "    print(train_fold_x.shape, valid_fold_x.shape)\n",
    "\n",
    "    model.fit(train_fold_x, train_fold_y)\n",
    "\n",
    "    # save model\n",
    "    with open(f\"./assets/model05_{fold}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    train_pred = model.predict(train_fold_x)\n",
    "    valid_pred = model.predict(valid_fold_x)\n",
    "\n",
    "    train_kappa = quadratic_weighted_kappa(\n",
    "        train_fold_y, train_pred.round(0).astype(int)\n",
    "    )\n",
    "    val_kappa = quadratic_weighted_kappa(valid_fold_y, valid_pred.round(0).astype(int))\n",
    "    cv_scores.append(val_kappa)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\"\n",
    "    )\n",
    "\n",
    "    for i, id_ in enumerate(fold_valid_ids):\n",
    "        oof.append({\"id\": id_, \"sii\": valid_pred[i]})\n",
    "\n",
    "    if y is None:\n",
    "        y = valid_fold[[\"id\", \"sii\"]]\n",
    "    else:\n",
    "        y = pd.concat([y, valid_fold[[\"id\", \"sii\"]]], axis=0).reset_index(drop=True)\n",
    "\n",
    "oof = pd.DataFrame(oof)\n",
    "\n",
    "KappaOPtimizer = minimize(\n",
    "    evaluate_predictions,\n",
    "    x0=[0.5, 1.5, 2.5],\n",
    "    args=(y[\"sii\"].astype(int), oof[\"sii\"]),\n",
    "    method=\"Nelder-Mead\",\n",
    ")\n",
    "\n",
    "oof_tuned = threshold_Rounder(oof[\"sii\"], KappaOPtimizer.x)\n",
    "tKappa = quadratic_weighted_kappa(y[\"sii\"], oof_tuned)\n",
    "print(f\"CV: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"tuned Kappa: {tKappa:.3f}\")  # tuned Kappa: 0.451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof.to_csv(\"./oof/oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
