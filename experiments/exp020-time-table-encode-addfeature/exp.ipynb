{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.dataloader_ import *\n",
    "from src.network_ import *\n",
    "from src.utils import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "train_series_dir = \"../../inputs/series_train.parquet/\"\n",
    "test_series_dir = \"../../inputs/series_test.parquet/\"\n",
    "\n",
    "data_dic_path = \"../../inputs/data_dictionary.csv\"\n",
    "sample_submission_path = \"../../inputs/sample_submission.csv\"\n",
    "train_path = \"../../inputs/train.csv\"\n",
    "test_path = \"../../inputs/test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "data_dic = pd.read_csv(data_dic_path)\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_torch(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imputer = KNNImputer(n_neighbors=5)\n",
    "sii_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "numeric_cols = test.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "numeric_feature_cols = numeric_cols.copy()\n",
    "# numeric_feature_cols = numeric_feature_cols.drop(\"sii\")\n",
    "\n",
    "numeric_sii_cols = train.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "feature_imputer.fit(test[numeric_feature_cols])\n",
    "feature_inputed = feature_imputer.fit_transform(train[numeric_feature_cols])\n",
    "\n",
    "train_imputed = pd.DataFrame(feature_inputed, columns=numeric_feature_cols)\n",
    "\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train[col]\n",
    "\n",
    "train = train_imputed\n",
    "\n",
    "\n",
    "sii_inputed = sii_imputer.fit_transform(train[numeric_sii_cols])\n",
    "\n",
    "sii_impute = pd.DataFrame(sii_inputed, columns=numeric_sii_cols)\n",
    "sii_impute[\"sii\"] = sii_impute[\"sii\"].round().astype(int)\n",
    "train[\"sii\"] = sii_impute[\"sii\"]\n",
    "\n",
    "with open(\"feature_imputer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_imputer, f)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"sii\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブルデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = [\n",
    "    \"BMI_Age\",\n",
    "    \"Internet_Hours_Age\",\n",
    "    \"BMI_Internet_Hours\",\n",
    "    \"BFP_BMI\",\n",
    "    \"FFMI_BFP\",\n",
    "    \"FMI_BFP\",\n",
    "    \"LST_TBW\",\n",
    "    \"BFP_BMR\",\n",
    "    \"BFP_DEE\",\n",
    "    \"BMR_Weight\",\n",
    "    \"DEE_Weight\",\n",
    "    \"SMM_Height\",\n",
    "    \"Muscle_to_Fat\",\n",
    "    \"Hydration_Status\",\n",
    "    \"ICW_TBW\",\n",
    "]\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # season_cols = [col for col in df.columns if \"Season\" in col]\n",
    "    # df = df.drop(season_cols, axis=1)\n",
    "    df[\"BMI_Age\"] = df[\"Physical-BMI\"] * df[\"Basic_Demos-Age\"]\n",
    "    df[\"Internet_Hours_Age\"] = (\n",
    "        df[\"PreInt_EduHx-computerinternet_hoursday\"] * df[\"Basic_Demos-Age\"]\n",
    "    )\n",
    "    df[\"BMI_Internet_Hours\"] = (\n",
    "        df[\"Physical-BMI\"] * df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "    )\n",
    "    df[\"BFP_BMI\"] = df[\"BIA-BIA_Fat\"] / df[\"BIA-BIA_BMI\"]\n",
    "    df[\"FFMI_BFP\"] = df[\"BIA-BIA_FFMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"FMI_BFP\"] = df[\"BIA-BIA_FMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"LST_TBW\"] = df[\"BIA-BIA_LST\"] / df[\"BIA-BIA_TBW\"]\n",
    "    df[\"BFP_BMR\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_BMR\"]\n",
    "    df[\"BFP_DEE\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_DEE\"]\n",
    "    df[\"BMR_Weight\"] = df[\"BIA-BIA_BMR\"] / df[\"Physical-Weight\"]\n",
    "    df[\"DEE_Weight\"] = df[\"BIA-BIA_DEE\"] / df[\"Physical-Weight\"]\n",
    "    df[\"SMM_Height\"] = df[\"BIA-BIA_SMM\"] / df[\"Physical-Height\"]\n",
    "    df[\"Muscle_to_Fat\"] = df[\"BIA-BIA_SMM\"] / df[\"BIA-BIA_FMI\"]\n",
    "    df[\"Hydration_Status\"] = df[\"BIA-BIA_TBW\"] / df[\"Physical-Weight\"]\n",
    "    df[\"ICW_TBW\"] = df[\"BIA-BIA_ICW\"] / df[\"BIA-BIA_TBW\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "for add_ in add_features:\n",
    "    train[add_] = train[add_].fillna(0.0)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehotEncoderの作成\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "categorical_columns = [\n",
    "    \"Basic_Demos-Enroll_Season\",\n",
    "    \"CGAS-Season\",\n",
    "    \"Physical-Season\",\n",
    "    \"PAQ_C-Season\",\n",
    "    \"FGC-Season\",\n",
    "    \"Fitness_Endurance-Season\",\n",
    "    \"PAQ_A-Season\",\n",
    "    \"BIA-Season\",\n",
    "    \"SDS-Season\",\n",
    "    \"PreInt_EduHx-Season\",\n",
    "]\n",
    "\n",
    "double_columns = [\n",
    "    \"FGC-FGC_SRR_Zone\",\n",
    "    \"BIA-BIA_SMM\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"BIA-BIA_FFMI\",\n",
    "    \"FGC-FGC_CU\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"BIA-BIA_ECW\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "    \"FGC-FGC_SRL_Zone\",\n",
    "    \"BIA-BIA_DEE\",\n",
    "    \"Physical-Weight\",\n",
    "    \"Fitness_Endurance-Time_Mins\",\n",
    "    \"FGC-FGC_SRR\",\n",
    "    \"SDS-SDS_Total_T\",\n",
    "    \"FGC-FGC_PU\",\n",
    "    \"BIA-BIA_FFM\",\n",
    "    \"FGC-FGC_TL_Zone\",\n",
    "    \"Physical-BMI\",\n",
    "    \"Physical-Systolic_BP\",\n",
    "    \"Physical-HeartRate\",\n",
    "    \"BIA-BIA_ICW\",\n",
    "    \"Physical-Height\",\n",
    "    \"FGC-FGC_SRL\",\n",
    "    \"BIA-BIA_BMC\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"BIA-BIA_Frame_num\",\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"FGC-FGC_GSND_Zone\",\n",
    "    \"Basic_Demos-Sex\",\n",
    "    \"FGC-FGC_GSND\",\n",
    "    \"BIA-BIA_LST\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"BIA-BIA_BMI\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"PAQ_C-PAQ_C_Total\",\n",
    "    \"BIA-BIA_Activity_Level_num\",\n",
    "    \"FGC-FGC_GSD\",\n",
    "    \"BIA-BIA_BMR\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"CGAS-CGAS_Score\",\n",
    "    \"FGC-FGC_PU_Zone\",\n",
    "    \"BIA-BIA_LDM\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"PAQ_A-PAQ_A_Total\",\n",
    "    \"BIA-BIA_TBW\",\n",
    "    \"FGC-FGC_GSD_Zone\",\n",
    "    \"Physical-Diastolic_BP\",\n",
    "]\n",
    "\n",
    "###################### categorical columns ######################\n",
    "# trainのtargetをonehot化\n",
    "onehot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "onehot_encoder.fit(train[categorical_columns])\n",
    "\n",
    "with open(\"./assets/onehot_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(onehot_encoder, f)\n",
    "\n",
    "categorical_feature = onehot_encoder.transform(train[categorical_columns])\n",
    "\n",
    "###################### double columns ######################\n",
    "# trainのtargetを標準化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[double_columns + add_features])\n",
    "\n",
    "with open(\"./assets/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "double_feature = scaler.transform(train[double_columns + add_features])\n",
    "# double_feature = train[double_columns].values\n",
    "\n",
    "# 欠損値の補完\n",
    "double_feature = np.nan_to_num(double_feature)\n",
    "\n",
    "###################### inputの作成 ######################\n",
    "\n",
    "ids = train[\"id\"].values.reshape(-1, 1)\n",
    "X = np.concatenate([categorical_feature, double_feature], axis=1)\n",
    "y = train[\"sii\"].fillna(-1).values.reshape(-1, 1)\n",
    "\n",
    "# DataFrameの作成\n",
    "ids_df = pd.DataFrame(ids, columns=[\"id\"])\n",
    "X_df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "y_df = pd.DataFrame(y, columns=[\"sii\"])\n",
    "\n",
    "train_df = pd.concat([ids_df, X_df, y_df], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(base_dir, id_):\n",
    "    path = os.path.join(base_dir, f\"id={id_}\", \"part-0.parquet\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def get_valid_ids(base_dir):\n",
    "    return [f.split(\"=\")[1].split(\".\")[0] for f in os.listdir(base_dir)]\n",
    "\n",
    "\n",
    "p = read_parquet(base_dir=\"../../inputs/series_train.parquet/\", id_=\"ffcd4dbd\")\n",
    "# p = read_parquet(base_dir=\"../../inputs/series_train.parquet/\", id_=\"10e46254\")\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# len(glob(\"../../normalized/*\"))\n",
    "len(glob(\"../../inputs/series_train.parquet/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(\n",
    "        oof_non_rounded < thresholds[0],\n",
    "        0,\n",
    "        np.where(\n",
    "            oof_non_rounded < thresholds[1],\n",
    "            1,\n",
    "            np.where(oof_non_rounded < thresholds[2], 2, 3),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "use_ids = list(\n",
    "    train_df[train_df[\"sii\"] != -1][\"id\"].unique()\n",
    ")  # get_valid_ids(base_dir=\"../../normalized/\")\n",
    "\n",
    "len(use_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "use_ids = np.array(use_ids)\n",
    "for train_index, valid_index in kf.split(use_ids):\n",
    "    train_ids = [use_ids[i] for i in train_index]\n",
    "    valid_ids = [use_ids[i] for i in valid_index]\n",
    "\n",
    "    train_dataset = CMIDataset(\n",
    "        table_df=train_df,\n",
    "        valid_ids=use_ids,\n",
    "        base_dir=\"../../inputs/series_train.parquet/\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][\"time_input\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "CV = []\n",
    "\n",
    "# use_ids = np.array(use_ids[:30]) # debug\n",
    "use_ids = np.array(use_ids)\n",
    "\n",
    "extract_df = train[train[\"id\"].isin(use_ids)].reset_index(drop=True)\n",
    "\n",
    "test_df = train[[\"id\", \"sii\"]].copy()\n",
    "# test_df[\"pred_sii\"] = 0\n",
    "oof_preds = []\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(\n",
    "    skf.split(extract_df[\"id\"], extract_df[\"sii\"])\n",
    "):\n",
    "    print(f\"################### fold:{fold} ###################\")\n",
    "    best_valid_score = -100\n",
    "\n",
    "    train_ids = use_ids[train_ids]\n",
    "    valid_ids = use_ids[valid_ids]\n",
    "\n",
    "    train_dataset = CMIDataset(\n",
    "        table_df=train_df,\n",
    "        valid_ids=train_ids,\n",
    "        base_dir=\"../../inputs/series_train.parquet/\",\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=30)\n",
    "\n",
    "    vlaid_dataset = CMIDataset(\n",
    "        table_df=train_df,\n",
    "        valid_ids=valid_ids,\n",
    "        base_dir=\"../../inputs/series_train.parquet/\",\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        vlaid_dataset, batch_size=1, shuffle=False, num_workers=30\n",
    "    )\n",
    "    # data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # model = TimeEncoder(input_size=26, hidden_size=13, num_layers=2).to(\"cuda\")\n",
    "    model = CMIModel(input_size=26, hidden_size=13, num_layers=2).to(\"cuda\")\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    spot_oof_preds = []\n",
    "\n",
    "    for epoch in range(5):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "\n",
    "        train_pred = []\n",
    "        valid_pred = []\n",
    "        trian_gt = []\n",
    "        valid_gt = []\n",
    "\n",
    "        tq = tqdm(train_loader)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            table_input = data[\"table_input\"].to(\"cuda\")\n",
    "            time_input = data[\"time_input\"].to(\"cuda\")\n",
    "            mask = data[\"mask\"].to(\"cuda\").to(torch.float32)\n",
    "            target_ = data[\"output\"].to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            output, attention_weight = model(table_input, time_input, active_mask=mask)\n",
    "            loss = criterion(output, target_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            train_pred.append(output.detach().cpu().numpy())\n",
    "            trian_gt.append(target_.detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix(loss=total_train_loss / (i + 1))\n",
    "            tq.update()\n",
    "        tq.close()\n",
    "\n",
    "        tq = tqdm(valid_loader)\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            table_input = data[\"table_input\"].to(\"cuda\")\n",
    "            time_input = data[\"time_input\"].to(\"cuda\")\n",
    "            mask = data[\"mask\"].to(\"cuda\").to(torch.float32)\n",
    "            target_ = data[\"output\"].to(\"cuda\")\n",
    "            output, attention_weight = model(table_input, time_input, active_mask=mask)\n",
    "            loss = criterion(output, target_)\n",
    "            total_valid_loss += loss.item()\n",
    "\n",
    "            valid_pred.append(output.detach().cpu().numpy())\n",
    "            valid_gt.append(target_.detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix(loss=total_valid_loss / (i + 1))\n",
    "            tq.update()\n",
    "        tq.close()\n",
    "\n",
    "        metric_train_pred = np.concatenate(train_pred)\n",
    "        metric_valid_pred = np.concatenate(valid_pred)\n",
    "        metric_train_gt = np.concatenate(trian_gt)\n",
    "        metric_valid_gt = np.concatenate(valid_gt)\n",
    "\n",
    "        train_score = quadratic_weighted_kappa(\n",
    "            metric_train_gt, metric_train_pred.round(0).astype(int)\n",
    "        )\n",
    "\n",
    "        valid_score = quadratic_weighted_kappa(\n",
    "            metric_valid_gt, metric_valid_pred.round(0).astype(int)\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"epoch: {epoch}, loss: {total_train_loss / len(train_loader)}, valid_loss: {total_valid_loss / len(valid_loader)}, train_score: {train_score}, valid_score: {valid_score}\"\n",
    "        )\n",
    "\n",
    "        if valid_score > best_valid_score:\n",
    "            best_valid_score = valid_score\n",
    "            torch.save(model.state_dict(), f\"./assets/model_{fold}.pth\")\n",
    "\n",
    "            spot_oof_preds = []\n",
    "            for i, id_ in enumerate(valid_ids):\n",
    "                spot_oof_preds.append({\"id\": id_, \"pred_sii\": valid_pred[i][0][0]})\n",
    "\n",
    "    oof_preds.append(spot_oof_preds)\n",
    "    CV.append(best_valid_score)\n",
    "\n",
    "print(f\"CV: {np.mean(CV)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds_df = pd.concat([pd.DataFrame(p) for p in oof_preds], axis=0).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "oof_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df = test_df.merge(oof_preds_df, on=\"id\", how=\"inner\")\n",
    "test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "KappaOPtimizer = minimize(\n",
    "    evaluate_predictions,\n",
    "    x0=[0.5, 1.5, 2.5],\n",
    "    args=(test_pred_df[\"sii\"], test_pred_df[\"pred_sii\"]),\n",
    "    method=\"Nelder-Mead\",\n",
    ")\n",
    "assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "oof_tuned = threshold_Rounder(test_pred_df[\"pred_sii\"], KappaOPtimizer.x)\n",
    "tKappa = quadratic_weighted_kappa(test_pred_df[\"sii\"], oof_tuned)\n",
    "print(f\"tuned Kappa: {tKappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KappaOPtimizer.x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
