{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.dataloader_ import *\n",
    "from src.network_ import *\n",
    "from src.utils import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "train_series_dir = \"../../inputs/series_train.parquet/\"\n",
    "test_series_dir = \"../../inputs/series_test.parquet/\"\n",
    "\n",
    "data_dic_path = \"../../inputs/data_dictionary.csv\"\n",
    "sample_submission_path = \"../../inputs/sample_submission.csv\"\n",
    "train_path = \"../../inputs/train.csv\"\n",
    "test_path = \"../../inputs/test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "data_dic = pd.read_csv(data_dic_path)\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "nb_name = os.path.basename(os.getcwd())  # notebook name\n",
    "seed_torch(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 24, 60, 15) (31, 24, 60, 15)\n",
      "(31, 24, 15) (31, 24, 15) (31, 24, 30)\n",
      "(744, 30)\n",
      "torch.Size([1, 744, 30])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def read_parquet(base_dir, id_):\n",
    "    path = os.path.join(base_dir, f\"id={id_}\", \"part-0.parquet\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def get_valid_ids(base_dir):\n",
    "    return [f.split(\"=\")[1].split(\".\")[0] for f in os.listdir(base_dir)]\n",
    "\n",
    "\n",
    "p = read_parquet(base_dir=\"../../inputs/series_train.parquet/\", id_=\"ffcd4dbd\")\n",
    "# p = read_parquet(base_dir=\"../../inputs/series_train.parquet/\", id_=\"10e46254\")\n",
    "\n",
    "scale_columns = [\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"enmo\",\n",
    "    \"anglez\",\n",
    "    \"light\",\n",
    "    \"battery_voltage\",\n",
    "]\n",
    "\n",
    "masked_columns = [\n",
    "    \"masked_X\",\n",
    "    \"masked_Y\",\n",
    "    \"masked_Z\",\n",
    "    \"masked_enmo\",\n",
    "    \"masked_anglez\",\n",
    "    \"masked_light\",\n",
    "]\n",
    "\n",
    "original_columns = [\"battery_voltage\", \"non-wear_flag\"]\n",
    "\n",
    "p[\"non-wear_flag\"] = 1 - p[\"non-wear_flag\"]\n",
    "scaler_features = p[scale_columns].values\n",
    "scaler = StandardScaler()\n",
    "p[scale_columns] = scaler.fit_transform(scaler_features)\n",
    "\n",
    "for mask_col in masked_columns:\n",
    "    p[mask_col] = p[mask_col.replace(\"masked_\", \"\")] * p[\"non-wear_flag\"]\n",
    "\n",
    "p = p.fillna(0.0)\n",
    "\n",
    "groups = p.groupby(\"relative_date_PCIAT\")\n",
    "# グループごとにデータフレームのリストに分割\n",
    "chunks = [group.reset_index(drop=True) for _, group in groups]\n",
    "\n",
    "use_cols = masked_columns + original_columns + scale_columns\n",
    "watch_day = len(chunks)\n",
    "active_logs = np.zeros((31, 17280, len(use_cols)), dtype=np.float32)\n",
    "active_mask = np.zeros((31), dtype=np.int32)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i == 0:  #\n",
    "        active_logs[i, -len(chunk) :, :] = chunk[use_cols].values\n",
    "    elif i == watch_day:\n",
    "        active_logs[i, : len(chunk), :] = chunk[use_cols].values\n",
    "    else:\n",
    "        array = chunk[use_cols].values\n",
    "        active_logs[i, : len(array), :] = array\n",
    "\n",
    "    active_mask[i] = 1\n",
    "\n",
    "    if i == 30:\n",
    "        break\n",
    "\n",
    "active_logs = active_logs.reshape(31, 24, 60, 12, 15)  # 12は1時間の分割数\n",
    "active_logs_mean = active_logs.mean(axis=3)  # 1時間の分割数で平均を取る # 31, 1440, 15\n",
    "# active_logs_var = active_logs.var(axis=3)  # 1時間の分割数で分散を取る # 31, 1440, 15\n",
    "active_logs = np.concatenate([active_logs_mean], axis=-1)  # (31, 24, 30)\n",
    "# print(active_logs_mean.shape, active_logs_var.shape, active_logs.shape)\n",
    "\n",
    "print(active_logs_mean.shape, active_logs.shape)\n",
    "\n",
    "active_logs_mean = active_logs.mean(axis=2)  # 1時間の分割数で平均を取る # 31, 1440, 15\n",
    "active_logs_var = active_logs.var(axis=2)  # 1時間の分割数で分散を取る # 31, 1440, 15\n",
    "active_logs = np.concatenate(\n",
    "    [active_logs_mean, active_logs_var], axis=-1\n",
    ")  # (31, 24, 30)\n",
    "print(active_logs_mean.shape, active_logs_var.shape, active_logs.shape)\n",
    "active_logs = active_logs.reshape(-1, 30)\n",
    "print(active_logs.shape)\n",
    "\n",
    "# active_logs = active_logs.unsqueeze(0)\n",
    "active_logs = torch.tensor(active_logs, dtype=torch.float32).unsqueeze(0).to(\"cuda\")\n",
    "print(active_logs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM AutoEncoder output shape: torch.Size([1, 744, 30]) torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class TransformerAutoEncoder(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=4, num_layers=2):\n",
    "        super(TransformerAutoEncoder, self).__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
    "        self.decoder = nn.TransformerDecoder(self.decoder_layer, num_layers=num_layers)\n",
    "        self.embedding = nn.Linear(30, d_model)\n",
    "        self.output_layer = nn.Linear(d_model, 30)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)  # (batch, day*time, d_model)\n",
    "        encoded = self.encoder(x.permute(1, 0, 2))  # (day*time, batch, d_model)\n",
    "        decoded = self.decoder(encoded, encoded)  # (day*time, batch, d_model)\n",
    "        return (\n",
    "            self.output_layer(decoded.permute(1, 0, 2)),\n",
    "            encoded,\n",
    "        )  # (batch, day*time, hidden)\n",
    "\n",
    "\n",
    "# Example\n",
    "model = TransformerAutoEncoder().to(\"cuda\")\n",
    "input_data = torch.randn(1, 744, 30).to(\"cuda\")\n",
    "output = model(input_data)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_size=30, hidden_size=64, num_layers=2):\n",
    "        super(LSTMAutoEncoder, self).__init__()\n",
    "        # Encoder LSTM\n",
    "        self.encoder_lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        # Decoder LSTM\n",
    "        self.decoder_lstm = nn.LSTM(\n",
    "            input_size=hidden_size,\n",
    "            hidden_size=input_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encode\n",
    "        _, (h, _) = self.encoder_lstm(x)\n",
    "        embedding = h[-1]\n",
    "        # Decode\n",
    "        h = (\n",
    "            h[-1].unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        )  # Repeat hidden state for each timestep\n",
    "        decoded, _ = self.decoder_lstm(h)\n",
    "        return decoded, embedding\n",
    "\n",
    "\n",
    "# 実行例\n",
    "model = LSTMAutoEncoder()\n",
    "input_data = torch.randn(1, 744, 30)\n",
    "output, embedding = model(input_data)\n",
    "print(\"LSTM AutoEncoder output shape:\", output.shape, embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN AutoEncoder output shape: torch.Size([1, 744, 30]) torch.Size([1, 128, 186])\n"
     ]
    }
   ],
   "source": [
    "class CNNAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNAutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=30, out_channels=64, kernel_size=3, stride=2, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(\n",
    "                64, 30, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, channel, time)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.permute(0, 2, 1), encoded  # (batch, time, channel)\n",
    "\n",
    "\n",
    "# 実行例\n",
    "model = CNNAutoEncoder()\n",
    "input_data = torch.randn(1, 744, 30)\n",
    "output, embedding = model(input_data)\n",
    "print(\"CNN AutoEncoder output shape:\", output.shape, embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各モデルのインスタンス化\n",
    "# transformer_model = TransformerAutoEncoder()\n",
    "\n",
    "# # 正規分布からランダムに(1, 31, 17280, 15)の形状でデータを生成\n",
    "# input_data = torch.randn(1, 31, 17280, 15)\n",
    "\n",
    "# # 各モデルにデータを入力し、出力形状を確認\n",
    "# print(\"Input shape:\", input_data.shape)\n",
    "\n",
    "# # Transformerモデル\n",
    "# transformer_output = transformer_model(input_data)\n",
    "# print(\"Transformer Model output shape:\", transformer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Basic_Demos-Enroll_Season</th>\n",
       "      <th>Basic_Demos-Age</th>\n",
       "      <th>Basic_Demos-Sex</th>\n",
       "      <th>CGAS-Season</th>\n",
       "      <th>CGAS-CGAS_Score</th>\n",
       "      <th>Physical-Season</th>\n",
       "      <th>Physical-BMI</th>\n",
       "      <th>Physical-Height</th>\n",
       "      <th>Physical-Weight</th>\n",
       "      <th>...</th>\n",
       "      <th>PCIAT-PCIAT_18</th>\n",
       "      <th>PCIAT-PCIAT_19</th>\n",
       "      <th>PCIAT-PCIAT_20</th>\n",
       "      <th>PCIAT-PCIAT_Total</th>\n",
       "      <th>SDS-Season</th>\n",
       "      <th>SDS-SDS_Total_Raw</th>\n",
       "      <th>SDS-SDS_Total_T</th>\n",
       "      <th>PreInt_EduHx-Season</th>\n",
       "      <th>PreInt_EduHx-computerinternet_hoursday</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>Fall</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>51.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>16.877316</td>\n",
       "      <td>46.0</td>\n",
       "      <td>50.8</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fall</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>Summer</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fall</td>\n",
       "      <td>14.035590</td>\n",
       "      <td>48.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>46.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>Summer</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>Fall</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>16.648696</td>\n",
       "      <td>56.5</td>\n",
       "      <td>75.6</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>38.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>Winter</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>18.292347</td>\n",
       "      <td>56.0</td>\n",
       "      <td>81.6</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Summer</td>\n",
       "      <td>31.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>Spring</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Summer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3955</th>\n",
       "      <td>ff8a2de4</td>\n",
       "      <td>Fall</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>60.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>16.362460</td>\n",
       "      <td>59.5</td>\n",
       "      <td>82.4</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>35.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3956</th>\n",
       "      <td>ffa9794a</td>\n",
       "      <td>Winter</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spring</td>\n",
       "      <td>18.764678</td>\n",
       "      <td>53.5</td>\n",
       "      <td>76.4</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Winter</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>ffcd4dbd</td>\n",
       "      <td>Fall</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>68.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>21.441500</td>\n",
       "      <td>60.0</td>\n",
       "      <td>109.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>56.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>Fall</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958</th>\n",
       "      <td>ffed1dd5</td>\n",
       "      <td>Spring</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Winter</td>\n",
       "      <td>12.235895</td>\n",
       "      <td>70.7</td>\n",
       "      <td>87.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>33.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>Spring</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>ffef538e</td>\n",
       "      <td>Spring</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Winter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spring</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3960 rows × 82 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id Basic_Demos-Enroll_Season  Basic_Demos-Age  Basic_Demos-Sex  \\\n",
       "0     00008ff9                      Fall                5                0   \n",
       "1     000fd460                    Summer                9                0   \n",
       "2     00105258                    Summer               10                1   \n",
       "3     00115b9f                    Winter                9                0   \n",
       "4     0016bb22                    Spring               18                1   \n",
       "...        ...                       ...              ...              ...   \n",
       "3955  ff8a2de4                      Fall               13                0   \n",
       "3956  ffa9794a                    Winter               10                0   \n",
       "3957  ffcd4dbd                      Fall               11                0   \n",
       "3958  ffed1dd5                    Spring               13                0   \n",
       "3959  ffef538e                    Spring               11                0   \n",
       "\n",
       "     CGAS-Season  CGAS-CGAS_Score Physical-Season  Physical-BMI  \\\n",
       "0         Winter             51.0            Fall     16.877316   \n",
       "1            NaN              NaN            Fall     14.035590   \n",
       "2           Fall             71.0            Fall     16.648696   \n",
       "3           Fall             71.0          Summer     18.292347   \n",
       "4         Summer              NaN             NaN           NaN   \n",
       "...          ...              ...             ...           ...   \n",
       "3955      Spring             60.0            Fall     16.362460   \n",
       "3956         NaN              NaN          Spring     18.764678   \n",
       "3957      Spring             68.0          Winter     21.441500   \n",
       "3958      Spring             70.0          Winter     12.235895   \n",
       "3959         NaN              NaN          Winter           NaN   \n",
       "\n",
       "      Physical-Height  Physical-Weight  ...  PCIAT-PCIAT_18  PCIAT-PCIAT_19  \\\n",
       "0                46.0             50.8  ...             4.0             2.0   \n",
       "1                48.0             46.0  ...             0.0             0.0   \n",
       "2                56.5             75.6  ...             2.0             1.0   \n",
       "3                56.0             81.6  ...             3.0             4.0   \n",
       "4                 NaN              NaN  ...             NaN             NaN   \n",
       "...               ...              ...  ...             ...             ...   \n",
       "3955             59.5             82.4  ...             1.0             1.0   \n",
       "3956             53.5             76.4  ...             NaN             NaN   \n",
       "3957             60.0            109.8  ...             1.0             0.0   \n",
       "3958             70.7             87.0  ...             1.0             1.0   \n",
       "3959              NaN              NaN  ...             NaN             NaN   \n",
       "\n",
       "      PCIAT-PCIAT_20  PCIAT-PCIAT_Total SDS-Season  SDS-SDS_Total_Raw  \\\n",
       "0                4.0               55.0        NaN                NaN   \n",
       "1                0.0                0.0       Fall               46.0   \n",
       "2                1.0               28.0       Fall               38.0   \n",
       "3                1.0               44.0     Summer               31.0   \n",
       "4                NaN                NaN        NaN                NaN   \n",
       "...              ...                ...        ...                ...   \n",
       "3955             0.0               32.0     Winter               35.0   \n",
       "3956             NaN                NaN        NaN                NaN   \n",
       "3957             1.0               31.0     Winter               56.0   \n",
       "3958             1.0               19.0     Spring               33.0   \n",
       "3959             NaN                NaN        NaN                NaN   \n",
       "\n",
       "      SDS-SDS_Total_T  PreInt_EduHx-Season  \\\n",
       "0                 NaN                 Fall   \n",
       "1                64.0               Summer   \n",
       "2                54.0               Summer   \n",
       "3                45.0               Winter   \n",
       "4                 NaN                  NaN   \n",
       "...               ...                  ...   \n",
       "3955             50.0                 Fall   \n",
       "3956              NaN               Winter   \n",
       "3957             77.0                 Fall   \n",
       "3958             47.0               Spring   \n",
       "3959              NaN               Spring   \n",
       "\n",
       "     PreInt_EduHx-computerinternet_hoursday  sii  \n",
       "0                                       3.0  2.0  \n",
       "1                                       0.0  0.0  \n",
       "2                                       2.0  0.0  \n",
       "3                                       0.0  1.0  \n",
       "4                                       NaN  NaN  \n",
       "...                                     ...  ...  \n",
       "3955                                    1.0  1.0  \n",
       "3956                                    0.0  NaN  \n",
       "3957                                    0.0  1.0  \n",
       "3958                                    1.0  0.0  \n",
       "3959                                    1.0  NaN  \n",
       "\n",
       "[3960 rows x 82 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブルデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_features = [\n",
    "    \"BMI_Age\",\n",
    "    \"Internet_Hours_Age\",\n",
    "    \"BMI_Internet_Hours\",\n",
    "    \"BFP_BMI\",\n",
    "    \"FFMI_BFP\",\n",
    "    \"FMI_BFP\",\n",
    "    \"LST_TBW\",\n",
    "    \"BFP_BMR\",\n",
    "    \"BFP_DEE\",\n",
    "    \"BMR_Weight\",\n",
    "    \"DEE_Weight\",\n",
    "    \"SMM_Height\",\n",
    "    \"Muscle_to_Fat\",\n",
    "    \"Hydration_Status\",\n",
    "    \"ICW_TBW\",\n",
    "]\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # season_cols = [col for col in df.columns if \"Season\" in col]\n",
    "    # df = df.drop(season_cols, axis=1)\n",
    "    df[\"BMI_Age\"] = df[\"Physical-BMI\"] * df[\"Basic_Demos-Age\"]\n",
    "    df[\"Internet_Hours_Age\"] = (\n",
    "        df[\"PreInt_EduHx-computerinternet_hoursday\"] * df[\"Basic_Demos-Age\"]\n",
    "    )\n",
    "    df[\"BMI_Internet_Hours\"] = (\n",
    "        df[\"Physical-BMI\"] * df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "    )\n",
    "    df[\"BFP_BMI\"] = df[\"BIA-BIA_Fat\"] / df[\"BIA-BIA_BMI\"]\n",
    "    df[\"FFMI_BFP\"] = df[\"BIA-BIA_FFMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"FMI_BFP\"] = df[\"BIA-BIA_FMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"LST_TBW\"] = df[\"BIA-BIA_LST\"] / df[\"BIA-BIA_TBW\"]\n",
    "    df[\"BFP_BMR\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_BMR\"]\n",
    "    df[\"BFP_DEE\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_DEE\"]\n",
    "    df[\"BMR_Weight\"] = df[\"BIA-BIA_BMR\"] / df[\"Physical-Weight\"]\n",
    "    df[\"DEE_Weight\"] = df[\"BIA-BIA_DEE\"] / df[\"Physical-Weight\"]\n",
    "    df[\"SMM_Height\"] = df[\"BIA-BIA_SMM\"] / df[\"Physical-Height\"]\n",
    "    df[\"Muscle_to_Fat\"] = df[\"BIA-BIA_SMM\"] / df[\"BIA-BIA_FMI\"]\n",
    "    df[\"Hydration_Status\"] = df[\"BIA-BIA_TBW\"] / df[\"Physical-Weight\"]\n",
    "    df[\"ICW_TBW\"] = df[\"BIA-BIA_ICW\"] / df[\"BIA-BIA_TBW\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "for add_ in add_features:\n",
    "    train[add_] = train[add_].fillna(0.0)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "\n",
    "test = feature_engineering(test)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "for add_ in add_features:\n",
    "    test[add_] = test[add_].fillna(0.0)\n",
    "test = test.dropna(thresh=10, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create new scaler\n"
     ]
    }
   ],
   "source": [
    "# onehotEncoderの作成\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "double_columns = [\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"Basic_Demos-Sex\",\n",
    "    \"CGAS-CGAS_Score\",\n",
    "    \"Physical-BMI\",\n",
    "    \"Physical-Height\",\n",
    "    \"Physical-Weight\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"Physical-Diastolic_BP\",\n",
    "    \"Physical-HeartRate\",\n",
    "    \"Physical-Systolic_BP\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"Fitness_Endurance-Time_Mins\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"FGC-FGC_CU\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "    \"FGC-FGC_GSND\",\n",
    "    \"FGC-FGC_GSND_Zone\",\n",
    "    \"FGC-FGC_GSD\",\n",
    "    \"FGC-FGC_GSD_Zone\",\n",
    "    \"FGC-FGC_PU\",\n",
    "    \"FGC-FGC_PU_Zone\",\n",
    "    \"FGC-FGC_SRL\",\n",
    "    \"FGC-FGC_SRL_Zone\",\n",
    "    \"FGC-FGC_SRR\",\n",
    "    \"FGC-FGC_SRR_Zone\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"FGC-FGC_TL_Zone\",\n",
    "    \"BIA-BIA_Activity_Level_num\",\n",
    "    \"BIA-BIA_BMC\",\n",
    "    \"BIA-BIA_BMI\",\n",
    "    \"BIA-BIA_BMR\",\n",
    "    \"BIA-BIA_DEE\",\n",
    "    \"BIA-BIA_ECW\",\n",
    "    \"BIA-BIA_FFM\",\n",
    "    \"BIA-BIA_FFMI\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"BIA-BIA_Frame_num\",\n",
    "    \"BIA-BIA_ICW\",\n",
    "    \"BIA-BIA_LDM\",\n",
    "    \"BIA-BIA_LST\",\n",
    "    \"BIA-BIA_SMM\",\n",
    "    \"BIA-BIA_TBW\",\n",
    "    \"PAQ_A-PAQ_A_Total\",\n",
    "    \"PAQ_C-PAQ_C_Total\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"SDS-SDS_Total_T\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "]\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def create_dataset_(df, scaler=None, train=True):\n",
    "\n",
    "    if scaler is None:\n",
    "        print(\"create new scaler\")\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(df[double_columns + add_features])\n",
    "        with open(\"./assets/scaler.pkl\", \"wb\") as f:\n",
    "            pickle.dump(scaler, f)\n",
    "\n",
    "    double_feature = scaler.transform(df[double_columns + add_features])\n",
    "    # 欠損値の補完\n",
    "    double_feature = np.nan_to_num(double_feature)\n",
    "\n",
    "    ids = df[\"id\"].values.reshape(-1, 1)\n",
    "    X = double_feature\n",
    "\n",
    "    # DataFrameの作成\n",
    "    ids_df = pd.DataFrame(ids, columns=[\"id\"])\n",
    "    X_df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    if train:\n",
    "        y = df[\"sii\"].fillna(-1).values.reshape(-1, 1)\n",
    "        y_df = pd.DataFrame(y, columns=[\"sii\"])\n",
    "        df = pd.concat([ids_df, X_df, y_df], axis=1)\n",
    "    else:\n",
    "        df = pd.concat([ids_df, X_df], axis=1)\n",
    "    return df, scaler\n",
    "\n",
    "\n",
    "train, scaler = create_dataset_(train)\n",
    "test = create_dataset_(test, scaler=scaler, train=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMIDataset(Dataset):\n",
    "    def __init__(self, table_df, valid_ids, base_dir, save_filename):\n",
    "        self.base_dir = base_dir\n",
    "        self.table_df = table_df\n",
    "        self.valid_ids = valid_ids\n",
    "        self.save_filename = save_filename\n",
    "        self.scale_columns = [\n",
    "            \"X\",\n",
    "            \"Y\",\n",
    "            \"Z\",\n",
    "            \"enmo\",\n",
    "            \"anglez\",\n",
    "            \"light\",\n",
    "            \"battery_voltage\",\n",
    "        ]\n",
    "\n",
    "        self.masked_columns = [\n",
    "            \"masked_X\",\n",
    "            \"masked_Y\",\n",
    "            \"masked_Z\",\n",
    "            \"masked_enmo\",\n",
    "            \"masked_anglez\",\n",
    "            \"masked_light\",\n",
    "        ]\n",
    "\n",
    "        self.original_columns = [\"battery_voltage\", \"non-wear_flag\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # テーブルデータの抽出\n",
    "        id_ = self.valid_ids[idx]\n",
    "\n",
    "        save_dir = f\"/home/tatsuya/code/projects/kaggle/ChildMindInstitute2024/precreated_dataset/{self.save_filename}/\"\n",
    "        save_path = os.path.join(save_dir, id_)\n",
    "\n",
    "        table = self.table_df.loc[self.table_df[\"id\"] == self.valid_ids[idx], :]\n",
    "        table_feature = table.drop(columns=[\"id\", \"sii\"]).values\n",
    "        sii = table[\"sii\"].values\n",
    "\n",
    "        # 時系列データの抽出\n",
    "        use_cols = self.masked_columns + self.original_columns + self.scale_columns\n",
    "        p = read_parquet(self.base_dir, self.valid_ids[idx])\n",
    "\n",
    "        if p is not None:\n",
    "            p[\"non-wear_flag\"] = 1 - p[\"non-wear_flag\"]\n",
    "            scaler_features = p[scale_columns].values\n",
    "            scaler = StandardScaler()\n",
    "            p[scale_columns] = scaler.fit_transform(scaler_features)\n",
    "\n",
    "            for mask_col in masked_columns:\n",
    "                p[mask_col] = p[mask_col.replace(\"masked_\", \"\")] * p[\"non-wear_flag\"]\n",
    "\n",
    "            p = p.fillna(0.0)\n",
    "\n",
    "            groups = p.groupby(\"relative_date_PCIAT\")\n",
    "            # グループごとにデータフレームのリストに分割\n",
    "            chunks = [group.reset_index(drop=True) for _, group in groups]\n",
    "\n",
    "            use_cols = masked_columns + original_columns + scale_columns\n",
    "            watch_day = len(chunks)\n",
    "            active_logs = np.zeros((31, 17280, len(use_cols)), dtype=np.float32)\n",
    "            active_mask = np.zeros((31), dtype=np.int32)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i == 0:  #\n",
    "                    active_logs[i, -len(chunk) :, :] = chunk[use_cols].values\n",
    "                elif i == watch_day:\n",
    "                    active_logs[i, : len(chunk), :] = chunk[use_cols].values\n",
    "                else:\n",
    "                    array = chunk[use_cols].values\n",
    "                    active_logs[i, : len(array), :] = array\n",
    "\n",
    "                active_mask[i] = 1\n",
    "\n",
    "                if i == 30:\n",
    "                    break\n",
    "\n",
    "            active_logs = active_logs.reshape(31, 24, 60, 12, 15)  # 12は1時間の分割数\n",
    "            active_logs_mean = active_logs.mean(\n",
    "                axis=3\n",
    "            )  # 1時間の分割数で平均を取る # 31, 1440, 15\n",
    "            # active_logs_var = active_logs.var(axis=3)  # 1時間の分割数で分散を取る # 31, 1440, 15\n",
    "            active_logs = np.concatenate([active_logs_mean], axis=-1)  # (31, 24, 30)\n",
    "            # print(active_logs_mean.shape, active_logs_var.shape, active_logs.shape)\n",
    "\n",
    "            # print(active_logs_mean.shape, active_logs.shape)\n",
    "\n",
    "            active_logs_mean = active_logs.mean(\n",
    "                axis=2\n",
    "            )  # 1時間の分割数で平均を取る # 31, 1440, 15\n",
    "            active_logs_var = active_logs.var(\n",
    "                axis=2\n",
    "            )  # 1時間の分割数で分散を取る # 31, 1440, 15\n",
    "            active_logs = np.concatenate(\n",
    "                [active_logs_mean, active_logs_var], axis=-1\n",
    "            )  # (31, 24, 30)\n",
    "            # print(active_logs_mean.shape, active_logs_var.shape, active_logs.shape)\n",
    "            active_logs = active_logs.reshape(-1, 30)\n",
    "\n",
    "        else:\n",
    "            active_logs = np.zeros((744, 30), dtype=np.float32)\n",
    "            active_mask = np.zeros((744), dtype=np.int32)\n",
    "\n",
    "        dataset_ = {\n",
    "            \"id\": id_,\n",
    "            \"table_input\": torch.tensor(table_feature, dtype=torch.float32),\n",
    "            \"time_input\": torch.tensor(active_logs, dtype=torch.float32),\n",
    "            \"mask\": torch.tensor(active_mask, dtype=torch.int32),\n",
    "            \"output\": torch.tensor(sii, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "        return dataset_\n",
    "\n",
    "\n",
    "def read_parquet(base_dir, id_):\n",
    "    path = os.path.join(base_dir, f\"id={id_}\", \"part-0.parquet\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "dataset = CMIDataset(\n",
    "    table_df=train,\n",
    "    valid_ids=get_valid_ids(train_series_dir),\n",
    "    base_dir=train_series_dir,\n",
    "    save_filename=\"train\",\n",
    ")\n",
    "\n",
    "# AutoEncoderのモデルのインスタンス化\n",
    "# transformer_model = TransformerAutoEncoder().to(\"cuda\")\n",
    "# transformer_model.load_state_dict(torch.load(\"./assets/transformer_autoencoder.pth\"))\n",
    "# lstm_model = LSTMAutoEncoder().to(\"cuda\")\n",
    "cnn_model = CNNAutoEncoder().to(\"cuda\")\n",
    "cnn_model.load_state_dict(torch.load(\"./assets/cnn_autoencoder.pth\"))\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.0001)\n",
    "# データセットからデータを取り出す\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_model = None\n",
    "minimum_loss = 1000000\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     print(f\"Epoch {epoch}\")\n",
    "#     dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "#     epoch_loss = []\n",
    "#     tq = tqdm(dataloader)\n",
    "#     for data in dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         table_input = data[\"table_input\"]\n",
    "#         time_input = data[\"time_input\"].to(\"cuda\")\n",
    "#         mask = data[\"mask\"]\n",
    "\n",
    "#         # モデルにデータを入力し、出力を取得\n",
    "#         cnn_output, embedding = cnn_model(time_input)\n",
    "#         # 損失の計算\n",
    "#         loss = criterion(cnn_output, time_input)\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss.append(loss.item())\n",
    "\n",
    "#         tq.set_postfix(loss=np.mean(epoch_loss))\n",
    "#         tq.update()\n",
    "\n",
    "#     if np.mean(epoch_loss) < minimum_loss:\n",
    "#         minimum_loss = np.mean(epoch_loss)\n",
    "#         best_model = cnn_model\n",
    "#         cnn_model.eval()\n",
    "#         torch.save(cnn_model.state_dict(), \"./assets/cnn_autoencoder.pth\")\n",
    "#         cnn_model.train()\n",
    "\n",
    "#     print(f\"Epoch {epoch} Loss: {np.mean(epoch_loss)}\")\n",
    "#     tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:30<00:00, 11.03it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = CMIDataset(\n",
    "    table_df=train,\n",
    "    valid_ids=get_valid_ids(train_series_dir),\n",
    "    base_dir=train_series_dir,\n",
    "    save_filename=\"train\",\n",
    ")\n",
    "\n",
    "# AutoEncoderのモデルのインスタンス化\n",
    "# transformer_model = TransformerAutoEncoder().to(\"cuda\")\n",
    "# transformer_model.load_state_dict(torch.load(\"./assets/transformer_autoencoder.pth\"))\n",
    "cnn_model = CNNAutoEncoder().to(\"cuda\")\n",
    "cnn_model.load_state_dict(torch.load(\"./assets/cnn_autoencoder.pth\"))\n",
    "# データセットからデータを取り出す\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_model = None\n",
    "minimum_loss = 1000000\n",
    "\n",
    "print(f\"Create Embedding\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "epoch_loss = []\n",
    "tq = tqdm(dataloader)\n",
    "\n",
    "embedding_result = []\n",
    "\n",
    "for data in dataloader:\n",
    "    id_ = data[\"id\"][0]\n",
    "    table_input = data[\"table_input\"]\n",
    "    time_input = data[\"time_input\"].to(\"cuda\")\n",
    "    mask = data[\"mask\"]\n",
    "\n",
    "    # モデルにデータを入力し、出力を取得\n",
    "    cnn_output, embedding = cnn_model(time_input)\n",
    "    # 損失の計算\n",
    "\n",
    "    mean_embedding = embedding.squeeze(0).mean(axis=-1).cpu().detach().numpy()\n",
    "    # mean_embedding = embedding.cpu().detach().numpy()\n",
    "\n",
    "    embedding_result.append({\"id\": id_, \"embedding\": mean_embedding})\n",
    "\n",
    "    tq.update()\n",
    "\n",
    "tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_118</th>\n",
       "      <th>embedding_119</th>\n",
       "      <th>embedding_120</th>\n",
       "      <th>embedding_121</th>\n",
       "      <th>embedding_122</th>\n",
       "      <th>embedding_123</th>\n",
       "      <th>embedding_124</th>\n",
       "      <th>embedding_125</th>\n",
       "      <th>embedding_126</th>\n",
       "      <th>embedding_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23dafdab</td>\n",
       "      <td>1.728628</td>\n",
       "      <td>1.331131</td>\n",
       "      <td>1.257535</td>\n",
       "      <td>1.118445</td>\n",
       "      <td>0.987019</td>\n",
       "      <td>1.087559</td>\n",
       "      <td>1.131072</td>\n",
       "      <td>0.995585</td>\n",
       "      <td>1.255212</td>\n",
       "      <td>...</td>\n",
       "      <td>1.302553</td>\n",
       "      <td>0.938135</td>\n",
       "      <td>0.814442</td>\n",
       "      <td>2.029479</td>\n",
       "      <td>1.367574</td>\n",
       "      <td>1.924622</td>\n",
       "      <td>1.463870</td>\n",
       "      <td>1.350334</td>\n",
       "      <td>2.242436</td>\n",
       "      <td>2.051528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e4614ec6</td>\n",
       "      <td>2.314725</td>\n",
       "      <td>1.688984</td>\n",
       "      <td>1.519484</td>\n",
       "      <td>1.735348</td>\n",
       "      <td>1.193666</td>\n",
       "      <td>1.536869</td>\n",
       "      <td>1.550757</td>\n",
       "      <td>1.390435</td>\n",
       "      <td>1.811116</td>\n",
       "      <td>...</td>\n",
       "      <td>1.619692</td>\n",
       "      <td>1.358370</td>\n",
       "      <td>1.046536</td>\n",
       "      <td>2.621617</td>\n",
       "      <td>1.780002</td>\n",
       "      <td>2.258282</td>\n",
       "      <td>2.161840</td>\n",
       "      <td>1.815999</td>\n",
       "      <td>3.007010</td>\n",
       "      <td>2.582609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ef356c</td>\n",
       "      <td>2.159015</td>\n",
       "      <td>1.613339</td>\n",
       "      <td>1.231065</td>\n",
       "      <td>1.499338</td>\n",
       "      <td>1.035320</td>\n",
       "      <td>1.470763</td>\n",
       "      <td>1.512265</td>\n",
       "      <td>1.431377</td>\n",
       "      <td>1.769337</td>\n",
       "      <td>...</td>\n",
       "      <td>1.495479</td>\n",
       "      <td>0.998973</td>\n",
       "      <td>0.892488</td>\n",
       "      <td>2.294397</td>\n",
       "      <td>1.778150</td>\n",
       "      <td>2.344225</td>\n",
       "      <td>2.104154</td>\n",
       "      <td>1.832239</td>\n",
       "      <td>2.810589</td>\n",
       "      <td>2.482788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dcfcd574</td>\n",
       "      <td>2.620850</td>\n",
       "      <td>1.587554</td>\n",
       "      <td>1.519351</td>\n",
       "      <td>1.526752</td>\n",
       "      <td>1.178476</td>\n",
       "      <td>1.720902</td>\n",
       "      <td>1.462330</td>\n",
       "      <td>1.542035</td>\n",
       "      <td>2.176839</td>\n",
       "      <td>...</td>\n",
       "      <td>1.697147</td>\n",
       "      <td>1.257415</td>\n",
       "      <td>0.930574</td>\n",
       "      <td>2.445918</td>\n",
       "      <td>1.790776</td>\n",
       "      <td>2.725111</td>\n",
       "      <td>2.195935</td>\n",
       "      <td>2.077631</td>\n",
       "      <td>2.984224</td>\n",
       "      <td>2.751604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>338146bd</td>\n",
       "      <td>1.702317</td>\n",
       "      <td>1.146785</td>\n",
       "      <td>0.947993</td>\n",
       "      <td>1.113863</td>\n",
       "      <td>0.825012</td>\n",
       "      <td>1.121790</td>\n",
       "      <td>1.108613</td>\n",
       "      <td>1.016863</td>\n",
       "      <td>1.336934</td>\n",
       "      <td>...</td>\n",
       "      <td>1.141957</td>\n",
       "      <td>0.823104</td>\n",
       "      <td>0.675855</td>\n",
       "      <td>1.746763</td>\n",
       "      <td>1.330033</td>\n",
       "      <td>1.805014</td>\n",
       "      <td>1.522724</td>\n",
       "      <td>1.314972</td>\n",
       "      <td>2.145939</td>\n",
       "      <td>1.884954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2a9e0dee</td>\n",
       "      <td>2.158872</td>\n",
       "      <td>1.343930</td>\n",
       "      <td>1.242806</td>\n",
       "      <td>1.386952</td>\n",
       "      <td>0.977405</td>\n",
       "      <td>1.362802</td>\n",
       "      <td>1.344970</td>\n",
       "      <td>1.286474</td>\n",
       "      <td>1.639083</td>\n",
       "      <td>...</td>\n",
       "      <td>1.352335</td>\n",
       "      <td>1.042948</td>\n",
       "      <td>0.783055</td>\n",
       "      <td>2.138570</td>\n",
       "      <td>1.547521</td>\n",
       "      <td>2.187941</td>\n",
       "      <td>1.923652</td>\n",
       "      <td>1.662507</td>\n",
       "      <td>2.642068</td>\n",
       "      <td>2.314295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0eddd8e5</td>\n",
       "      <td>2.484117</td>\n",
       "      <td>1.468063</td>\n",
       "      <td>1.276847</td>\n",
       "      <td>1.519080</td>\n",
       "      <td>1.297554</td>\n",
       "      <td>1.425604</td>\n",
       "      <td>1.605949</td>\n",
       "      <td>1.533385</td>\n",
       "      <td>1.932054</td>\n",
       "      <td>...</td>\n",
       "      <td>1.541252</td>\n",
       "      <td>1.163978</td>\n",
       "      <td>0.947115</td>\n",
       "      <td>2.526440</td>\n",
       "      <td>1.801957</td>\n",
       "      <td>2.367331</td>\n",
       "      <td>2.304687</td>\n",
       "      <td>2.076990</td>\n",
       "      <td>2.887928</td>\n",
       "      <td>2.770179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a49eda7f</td>\n",
       "      <td>2.429227</td>\n",
       "      <td>1.728591</td>\n",
       "      <td>1.312529</td>\n",
       "      <td>1.631665</td>\n",
       "      <td>1.108303</td>\n",
       "      <td>1.534565</td>\n",
       "      <td>1.537943</td>\n",
       "      <td>1.418095</td>\n",
       "      <td>1.846664</td>\n",
       "      <td>...</td>\n",
       "      <td>1.545845</td>\n",
       "      <td>1.069983</td>\n",
       "      <td>0.899543</td>\n",
       "      <td>2.499636</td>\n",
       "      <td>1.783232</td>\n",
       "      <td>2.476268</td>\n",
       "      <td>2.219812</td>\n",
       "      <td>1.915287</td>\n",
       "      <td>3.120052</td>\n",
       "      <td>2.707149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fa34f945</td>\n",
       "      <td>1.557142</td>\n",
       "      <td>0.875563</td>\n",
       "      <td>1.173156</td>\n",
       "      <td>0.954732</td>\n",
       "      <td>0.988366</td>\n",
       "      <td>1.082507</td>\n",
       "      <td>1.034887</td>\n",
       "      <td>0.900241</td>\n",
       "      <td>1.134255</td>\n",
       "      <td>...</td>\n",
       "      <td>1.228432</td>\n",
       "      <td>0.866287</td>\n",
       "      <td>0.633695</td>\n",
       "      <td>1.558471</td>\n",
       "      <td>1.345874</td>\n",
       "      <td>1.759833</td>\n",
       "      <td>1.225766</td>\n",
       "      <td>1.281289</td>\n",
       "      <td>1.923254</td>\n",
       "      <td>1.814930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>526f719b</td>\n",
       "      <td>1.921495</td>\n",
       "      <td>1.219016</td>\n",
       "      <td>1.022068</td>\n",
       "      <td>1.226168</td>\n",
       "      <td>0.862962</td>\n",
       "      <td>1.178011</td>\n",
       "      <td>1.206993</td>\n",
       "      <td>1.087231</td>\n",
       "      <td>1.369162</td>\n",
       "      <td>...</td>\n",
       "      <td>1.198181</td>\n",
       "      <td>0.883189</td>\n",
       "      <td>0.728197</td>\n",
       "      <td>1.932107</td>\n",
       "      <td>1.345296</td>\n",
       "      <td>1.977191</td>\n",
       "      <td>1.667278</td>\n",
       "      <td>1.389008</td>\n",
       "      <td>2.317091</td>\n",
       "      <td>1.877849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "0   23dafdab     1.728628     1.331131     1.257535     1.118445     0.987019   \n",
       "0   e4614ec6     2.314725     1.688984     1.519484     1.735348     1.193666   \n",
       "0   56ef356c     2.159015     1.613339     1.231065     1.499338     1.035320   \n",
       "0   dcfcd574     2.620850     1.587554     1.519351     1.526752     1.178476   \n",
       "0   338146bd     1.702317     1.146785     0.947993     1.113863     0.825012   \n",
       "..       ...          ...          ...          ...          ...          ...   \n",
       "0   2a9e0dee     2.158872     1.343930     1.242806     1.386952     0.977405   \n",
       "0   0eddd8e5     2.484117     1.468063     1.276847     1.519080     1.297554   \n",
       "0   a49eda7f     2.429227     1.728591     1.312529     1.631665     1.108303   \n",
       "0   fa34f945     1.557142     0.875563     1.173156     0.954732     0.988366   \n",
       "0   526f719b     1.921495     1.219016     1.022068     1.226168     0.862962   \n",
       "\n",
       "    embedding_5  embedding_6  embedding_7  embedding_8  ...  embedding_118  \\\n",
       "0      1.087559     1.131072     0.995585     1.255212  ...       1.302553   \n",
       "0      1.536869     1.550757     1.390435     1.811116  ...       1.619692   \n",
       "0      1.470763     1.512265     1.431377     1.769337  ...       1.495479   \n",
       "0      1.720902     1.462330     1.542035     2.176839  ...       1.697147   \n",
       "0      1.121790     1.108613     1.016863     1.336934  ...       1.141957   \n",
       "..          ...          ...          ...          ...  ...            ...   \n",
       "0      1.362802     1.344970     1.286474     1.639083  ...       1.352335   \n",
       "0      1.425604     1.605949     1.533385     1.932054  ...       1.541252   \n",
       "0      1.534565     1.537943     1.418095     1.846664  ...       1.545845   \n",
       "0      1.082507     1.034887     0.900241     1.134255  ...       1.228432   \n",
       "0      1.178011     1.206993     1.087231     1.369162  ...       1.198181   \n",
       "\n",
       "    embedding_119  embedding_120  embedding_121  embedding_122  embedding_123  \\\n",
       "0        0.938135       0.814442       2.029479       1.367574       1.924622   \n",
       "0        1.358370       1.046536       2.621617       1.780002       2.258282   \n",
       "0        0.998973       0.892488       2.294397       1.778150       2.344225   \n",
       "0        1.257415       0.930574       2.445918       1.790776       2.725111   \n",
       "0        0.823104       0.675855       1.746763       1.330033       1.805014   \n",
       "..            ...            ...            ...            ...            ...   \n",
       "0        1.042948       0.783055       2.138570       1.547521       2.187941   \n",
       "0        1.163978       0.947115       2.526440       1.801957       2.367331   \n",
       "0        1.069983       0.899543       2.499636       1.783232       2.476268   \n",
       "0        0.866287       0.633695       1.558471       1.345874       1.759833   \n",
       "0        0.883189       0.728197       1.932107       1.345296       1.977191   \n",
       "\n",
       "    embedding_124  embedding_125  embedding_126  embedding_127  \n",
       "0        1.463870       1.350334       2.242436       2.051528  \n",
       "0        2.161840       1.815999       3.007010       2.582609  \n",
       "0        2.104154       1.832239       2.810589       2.482788  \n",
       "0        2.195935       2.077631       2.984224       2.751604  \n",
       "0        1.522724       1.314972       2.145939       1.884954  \n",
       "..            ...            ...            ...            ...  \n",
       "0        1.923652       1.662507       2.642068       2.314295  \n",
       "0        2.304687       2.076990       2.887928       2.770179  \n",
       "0        2.219812       1.915287       3.120052       2.707149  \n",
       "0        1.225766       1.281289       1.923254       1.814930  \n",
       "0        1.667278       1.389008       2.317091       1.877849  \n",
       "\n",
       "[996 rows x 129 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_df_all = None\n",
    "\n",
    "for row in embedding_result:\n",
    "    id_ = row[\"id\"]\n",
    "    embedding = row[\"embedding\"]\n",
    "    embedding_cols = [f\"embedding_{i}\" for i in range(embedding.shape[-1])]\n",
    "    embedding_df = pd.DataFrame(embedding.reshape(1, -1), columns=embedding_cols)\n",
    "    embedding_df[\"id\"] = id_\n",
    "\n",
    "    if embedding_df_all is None:\n",
    "        embedding_df_all = embedding_df\n",
    "    else:\n",
    "        embedding_df_all = pd.concat([embedding_df_all, embedding_df], axis=0)\n",
    "\n",
    "embedding_df_all = embedding_df_all[[\"id\"] + embedding_cols]\n",
    "embedding_df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(\n",
    "        oof_non_rounded < thresholds[0],\n",
    "        0,\n",
    "        np.where(\n",
    "            oof_non_rounded < thresholds[1],\n",
    "            1,\n",
    "            np.where(oof_non_rounded < thresholds[2], 2, 3),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train[\"sii\"] != -1].reset_index(drop=True)\n",
    "train = train.merge(embedding_df_all, on=\"id\", how=\"left\")\n",
    "# train.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_118</th>\n",
       "      <th>embedding_119</th>\n",
       "      <th>embedding_120</th>\n",
       "      <th>embedding_121</th>\n",
       "      <th>embedding_122</th>\n",
       "      <th>embedding_123</th>\n",
       "      <th>embedding_124</th>\n",
       "      <th>embedding_125</th>\n",
       "      <th>embedding_126</th>\n",
       "      <th>embedding_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>-1.520226</td>\n",
       "      <td>-0.770846</td>\n",
       "      <td>-0.647115</td>\n",
       "      <td>-0.480065</td>\n",
       "      <td>-1.331104</td>\n",
       "      <td>-0.858103</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.428142</td>\n",
       "      <td>1.037569</td>\n",
       "      <td>0.784901</td>\n",
       "      <td>2.058612</td>\n",
       "      <td>1.580202</td>\n",
       "      <td>2.141743</td>\n",
       "      <td>1.709615</td>\n",
       "      <td>1.614125</td>\n",
       "      <td>2.443338</td>\n",
       "      <td>2.158067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>-0.401093</td>\n",
       "      <td>-0.770846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.035839</td>\n",
       "      <td>-1.063457</td>\n",
       "      <td>-0.965819</td>\n",
       "      <td>-0.948658</td>\n",
       "      <td>0.393202</td>\n",
       "      <td>-0.848813</td>\n",
       "      <td>...</td>\n",
       "      <td>1.160853</td>\n",
       "      <td>0.845095</td>\n",
       "      <td>0.668588</td>\n",
       "      <td>1.787926</td>\n",
       "      <td>1.312256</td>\n",
       "      <td>1.872245</td>\n",
       "      <td>1.547791</td>\n",
       "      <td>1.371346</td>\n",
       "      <td>2.154535</td>\n",
       "      <td>1.842573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>-0.121310</td>\n",
       "      <td>1.297277</td>\n",
       "      <td>0.248250</td>\n",
       "      <td>-0.524777</td>\n",
       "      <td>0.074043</td>\n",
       "      <td>-0.301573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.341610</td>\n",
       "      <td>0.907770</td>\n",
       "      <td>...</td>\n",
       "      <td>1.404206</td>\n",
       "      <td>1.106323</td>\n",
       "      <td>0.821356</td>\n",
       "      <td>2.140345</td>\n",
       "      <td>1.584761</td>\n",
       "      <td>2.223369</td>\n",
       "      <td>1.837487</td>\n",
       "      <td>1.689799</td>\n",
       "      <td>2.566232</td>\n",
       "      <td>2.280330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>-0.401093</td>\n",
       "      <td>-0.770846</td>\n",
       "      <td>0.248250</td>\n",
       "      <td>-0.203318</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>-0.166928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.709017</td>\n",
       "      <td>1.127343</td>\n",
       "      <td>...</td>\n",
       "      <td>0.635673</td>\n",
       "      <td>0.428215</td>\n",
       "      <td>0.316652</td>\n",
       "      <td>0.767731</td>\n",
       "      <td>0.634796</td>\n",
       "      <td>0.851805</td>\n",
       "      <td>0.541776</td>\n",
       "      <td>0.496939</td>\n",
       "      <td>0.895844</td>\n",
       "      <td>0.785507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>0.718039</td>\n",
       "      <td>1.297277</td>\n",
       "      <td>-0.691883</td>\n",
       "      <td>0.576564</td>\n",
       "      <td>0.475513</td>\n",
       "      <td>0.519759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.709017</td>\n",
       "      <td>-0.629240</td>\n",
       "      <td>...</td>\n",
       "      <td>1.504753</td>\n",
       "      <td>0.889019</td>\n",
       "      <td>0.727498</td>\n",
       "      <td>1.811945</td>\n",
       "      <td>1.743254</td>\n",
       "      <td>2.050748</td>\n",
       "      <td>1.620137</td>\n",
       "      <td>1.588830</td>\n",
       "      <td>2.485625</td>\n",
       "      <td>2.320382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0  00008ff9  -1.520226  -0.770846  -0.647115  -0.480065  -1.331104  -0.858103   \n",
       "1  000fd460  -0.401093  -0.770846   0.000000  -1.035839  -1.063457  -0.965819   \n",
       "2  00105258  -0.121310   1.297277   0.248250  -0.524777   0.074043  -0.301573   \n",
       "3  00115b9f  -0.401093  -0.770846   0.248250  -0.203318   0.007131  -0.166928   \n",
       "4  001f3379   0.718039   1.297277  -0.691883   0.576564   0.475513   0.519759   \n",
       "\n",
       "   feature_6  feature_7  feature_8  ...  embedding_118  embedding_119  \\\n",
       "0   0.000000   0.000000   0.000000  ...       1.428142       1.037569   \n",
       "1  -0.948658   0.393202  -0.848813  ...       1.160853       0.845095   \n",
       "2   0.000000  -0.341610   0.907770  ...       1.404206       1.106323   \n",
       "3   0.000000  -0.709017   1.127343  ...       0.635673       0.428215   \n",
       "4   0.000000  -0.709017  -0.629240  ...       1.504753       0.889019   \n",
       "\n",
       "   embedding_120  embedding_121  embedding_122  embedding_123  embedding_124  \\\n",
       "0       0.784901       2.058612       1.580202       2.141743       1.709615   \n",
       "1       0.668588       1.787926       1.312256       1.872245       1.547791   \n",
       "2       0.821356       2.140345       1.584761       2.223369       1.837487   \n",
       "3       0.316652       0.767731       0.634796       0.851805       0.541776   \n",
       "4       0.727498       1.811945       1.743254       2.050748       1.620137   \n",
       "\n",
       "   embedding_125  embedding_126  embedding_127  \n",
       "0       1.614125       2.443338       2.158067  \n",
       "1       1.371346       2.154535       1.842573  \n",
       "2       1.689799       2.566232       2.280330  \n",
       "3       0.496939       0.895844       0.785507  \n",
       "4       1.588830       2.485625       2.320382  \n",
       "\n",
       "[5 rows x 193 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imputer = KNNImputer(n_neighbors=5)\n",
    "sii_imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "numeric_cols = train.columns.drop(\n",
    "    [\"id\", \"sii\"]\n",
    ")  # test.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "numeric_feature_cols = numeric_cols.copy()\n",
    "# numeric_feature_cols = numeric_feature_cols.drop(\"sii\")\n",
    "\n",
    "# numeric_sii_cols = train.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "# sii_inputed = sii_imputer.fit_transform(train[numeric_sii_cols])\n",
    "\n",
    "train[numeric_cols] = feature_imputer.fit_transform(train[numeric_feature_cols])\n",
    "\n",
    "with open(\"feature_imputer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(feature_imputer, f)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.fillna(0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  20%|██        | 1/5 [00:00<00:02,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train QWK: 0.7856, Validation QWK: 0.3923\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  40%|████      | 2/5 [00:00<00:01,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train QWK: 0.7970, Validation QWK: 0.4294\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  60%|██████    | 3/5 [00:01<00:00,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Train QWK: 0.8029, Validation QWK: 0.4079\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  80%|████████  | 4/5 [00:01<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Train QWK: 0.8035, Validation QWK: 0.3694\n",
      "[LightGBM] [Warning] lambda_l1 is set=10, reg_alpha=0.0 will be ignored. Current value: lambda_l1=10\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.784, subsample=1.0 will be ignored. Current value: bagging_fraction=0.784\n",
      "[LightGBM] [Warning] feature_fraction is set=0.893, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.893\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=13, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=13\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] lambda_l2 is set=0.01, reg_lambda=0.0 will be ignored. Current value: lambda_l2=0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:01<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train QWK: 0.8019, Validation QWK: 0.3606\n",
      "Mean Train QWK --> 0.7982\n",
      "CV: 0.3919\n",
      "tuned Kappa: 0.453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  20%|██        | 1/5 [00:00<00:02,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train QWK: 0.9233, Validation QWK: 0.3892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  40%|████      | 2/5 [00:01<00:01,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train QWK: 0.9206, Validation QWK: 0.4264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  60%|██████    | 3/5 [00:01<00:01,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Train QWK: 0.9282, Validation QWK: 0.3561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  80%|████████  | 4/5 [00:02<00:00,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Train QWK: 0.9319, Validation QWK: 0.3586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:03<00:00,  1.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train QWK: 0.9282, Validation QWK: 0.3519\n",
      "Mean Train QWK --> 0.9264\n",
      "CV: 0.3765\n",
      "tuned Kappa: 0.436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  20%|██        | 1/5 [00:00<00:02,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train QWK: 0.5194, Validation QWK: 0.3686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  40%|████      | 2/5 [00:01<00:02,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 - Train QWK: 0.5474, Validation QWK: 0.4242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  60%|██████    | 3/5 [00:02<00:01,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 - Train QWK: 0.5345, Validation QWK: 0.4020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds:  80%|████████  | 4/5 [00:03<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 - Train QWK: 0.5412, Validation QWK: 0.3329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:03<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 - Train QWK: 0.5465, Validation QWK: 0.3330\n",
      "Mean Train QWK --> 0.5378\n",
      "CV: 0.3721\n",
      "tuned Kappa: 0.462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "import polars.selectors as cs\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from keras.models import Model\n",
    "# from keras.layers import Input, Dense\n",
    "# from keras.optimizers import Adam\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (\n",
    "    VotingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(\n",
    "        oof_non_rounded < thresholds[0],\n",
    "        0,\n",
    "        np.where(\n",
    "            oof_non_rounded < thresholds[1],\n",
    "            1,\n",
    "            np.where(oof_non_rounded < thresholds[2], 2, 3),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "\n",
    "def TrainML(model_class, test_data, model_name):  # -> Series[Any] | Any:\n",
    "    id_ = train[\"id\"]\n",
    "    X = train.drop([\"sii\", \"id\"], axis=1)\n",
    "    y = train[\"sii\"]\n",
    "\n",
    "    y_df = train[[\"id\", \"sii\"]]\n",
    "\n",
    "    n_splits = 5\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "    train_S = []\n",
    "    test_S = []\n",
    "\n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float)\n",
    "    oof_rounded = np.zeros(len(y), dtype=int)\n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(\n",
    "        tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)\n",
    "    ):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        train_id = id_.iloc[train_idx]\n",
    "        val_id = id_.iloc[test_idx]\n",
    "\n",
    "        # 各idを保存\n",
    "        with open(f\"./assets/train_id_{fold}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(train_id, f)\n",
    "\n",
    "        with open(f\"./assets/val_id_{fold}.pkl\", \"wb\") as f:\n",
    "            pickle.dump(val_id, f)\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(\n",
    "            y_train, y_train_pred.round(0).astype(int)\n",
    "        )\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "\n",
    "        # test_preds[:, fold] = model.predict(test_data.drop(columns=[\"id\"]))\n",
    "\n",
    "        print(\n",
    "            f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\"\n",
    "        )\n",
    "        # clear_output(wait=True)\n",
    "\n",
    "        voting_model = model\n",
    "        # modelの保存\n",
    "\n",
    "        if model_name != \"lgbm\":\n",
    "            with open(f\"./assets/{model_name}_{fold}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(voting_model, f)\n",
    "        else:\n",
    "            # lgbmの場合\n",
    "            model.booster_.save_model(f\"./assets/{model_name}_{fold}.txt\")\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"CV: {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(\n",
    "        evaluate_predictions,\n",
    "        x0=[0.5, 1.5, 2.5],\n",
    "        args=(y, oof_non_rounded),\n",
    "        method=\"Nelder-Mead\",\n",
    "    )\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"tuned Kappa: {tKappa:.3f}\")\n",
    "\n",
    "    # tpm = test_preds.mean(axis=1)\n",
    "    # tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "    # y[\"pred\"] = oof_non_rounded.values\n",
    "    y_df[\"pred\"] = oof_non_rounded\n",
    "    return y_df\n",
    "\n",
    "\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    \"learning_rate\": 0.046,\n",
    "    \"max_depth\": 12,\n",
    "    \"num_leaves\": 478,\n",
    "    \"min_data_in_leaf\": 13,\n",
    "    \"feature_fraction\": 0.893,\n",
    "    \"bagging_fraction\": 0.784,\n",
    "    \"bagging_freq\": 4,\n",
    "    \"lambda_l1\": 10,  # Increased from 6.59\n",
    "    \"lambda_l2\": 0.01,  # Increased from 2.68e-06\n",
    "}\n",
    "\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"n_estimators\": 200,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 1,  # Increased from 0.1\n",
    "    \"reg_lambda\": 5,  # Increased from 1\n",
    "    \"random_state\": SEED,\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 6,\n",
    "    \"iterations\": 200,\n",
    "    \"random_seed\": SEED,\n",
    "    \"verbose\": 0,\n",
    "    \"l2_leaf_reg\": 10,  # Increase this value\n",
    "}\n",
    "\n",
    "# Create model instances\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "lgbm = Light\n",
    "xgb = XGB_Model\n",
    "ctb = CatBoost_Model\n",
    "\n",
    "# Train the ensemble model\n",
    "lgbm_oof = TrainML(lgbm, test, model_name=\"lgbm\")\n",
    "xgb_oof = TrainML(xgb, test, model_name=\"xgb\")\n",
    "ctb_oof = TrainML(ctb, test, model_name=\"ctb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./assets/lstm_oof_preds.pkl\", \"rb\") as f:\n",
    "    lstm_oof = pickle.load(f)\n",
    "\n",
    "lgbm_oof = lgbm_oof.sort_values(\"id\").reset_index(drop=True)\n",
    "xgb_oof = xgb_oof.sort_values(\"id\").reset_index(drop=True)\n",
    "ctb_oof = ctb_oof.sort_values(\"id\").reset_index(drop=True)\n",
    "lstm_oof = lstm_oof.sort_values(\"id\").reset_index(drop=True)\n",
    "\n",
    "oof = lstm_oof.rename(columns={\"pred_sii\": \"lstm_oof\"})\n",
    "oof[\"lgbm_oof\"] = lgbm_oof[\"pred\"]\n",
    "oof[\"xgb_oof\"] = xgb_oof[\"pred\"]\n",
    "oof[\"ctb_oof\"] = ctb_oof[\"pred\"]\n",
    "\n",
    "\n",
    "y = xgb_oof[\"sii\"].values\n",
    "oof = oof[[\"id\", \"lstm_oof\", \"lgbm_oof\", \"xgb_oof\", \"ctb_oof\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned Kappa: 0.471\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(\n",
    "        oof_non_rounded < thresholds[0],\n",
    "        0,\n",
    "        np.where(\n",
    "            oof_non_rounded < thresholds[1],\n",
    "            1,\n",
    "            np.where(oof_non_rounded < thresholds[2], 2, 3),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    mean_weight = thresholds[:4]\n",
    "    thresholds = thresholds[4:]\n",
    "\n",
    "    lstm_pred = oof_non_rounded[:, 0]\n",
    "    lgbm_pred = oof_non_rounded[:, 1]\n",
    "    xgb_pred = oof_non_rounded[:, 2]\n",
    "    ctb_pred = oof_non_rounded[:, 3]\n",
    "    oof_non_rounded = (\n",
    "        mean_weight[0] * lstm_pred\n",
    "        + mean_weight[1] * lgbm_pred\n",
    "        + mean_weight[2] * xgb_pred\n",
    "        + mean_weight[3] * ctb_pred\n",
    "    )\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "\n",
    "KappaOPtimizer = minimize(\n",
    "    evaluate_predictions,\n",
    "    x0=[0.25, 0.25, 0.25, 0.25, 0.5, 1.5, 2.5],\n",
    "    args=(y, oof.drop(columns=[\"id\"]).values),\n",
    "    method=\"Nelder-Mead\",\n",
    ")\n",
    "\n",
    "pred = oof.drop(columns=[\"id\"]).values\n",
    "pred = (\n",
    "    pred[:, 0] * KappaOPtimizer.x[0]\n",
    "    + pred[:, 1] * KappaOPtimizer.x[1]\n",
    "    + pred[:, 2] * KappaOPtimizer.x[2]\n",
    "    + pred[:, 3] * KappaOPtimizer.x[3]\n",
    ")\n",
    "\n",
    "oof_tuned = threshold_Rounder(pred, KappaOPtimizer.x[4:])\n",
    "tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "print(f\"tuned Kappa: {tKappa:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25978536, 0.25245911, 0.26024378, 0.25166782, 0.58545671,\n",
       "       1.09664355, 2.63238037])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KappaOPtimizer.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "model_path = f\"./assets/lgbm_0.txt\"\n",
    "loaded_model = lgb.Booster(model_file=model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99616248, 0.14798295, 0.42020302, ..., 1.09092573, 0.67819407,\n",
       "       0.43826146])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict(train.drop([\"id\", \"sii\"], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_submission[\"sii\"] = (\n",
    "    final_submission[\"lstm\"] * 0.25978536\n",
    "    + final_submission[\"lgbm\"] * 0.25245911\n",
    "    + final_submission[\"xgb\"] * 0.26024378\n",
    "    + final_submission[\"ctb\"] * 0.25166782\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
