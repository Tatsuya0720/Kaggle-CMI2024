{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from src.dataloader_ import *\n",
    "# from src.network_ import *\n",
    "from src.utils import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "train_series_dir = \"../../../inputs/series_train.parquet/\"\n",
    "test_series_dir = \"../../../inputs/series_test.parquet/\"\n",
    "\n",
    "data_dic_path = \"../../../inputs/data_dictionary.csv\"\n",
    "sample_submission_path = \"../../../inputs/sample_submission.csv\"\n",
    "train_path = \"../../../inputs/train.csv\"\n",
    "test_path = \"../../../inputs/test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "data_dic = pd.read_csv(data_dic_path)\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "nb_name = os.path.basename(os.getcwd())  # notebook name\n",
    "seed_torch(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの前処理\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "add_features = [\n",
    "    \"BMI_Age\",\n",
    "    \"Internet_Hours_Age\",\n",
    "    \"BMI_Internet_Hours\",\n",
    "    \"BFP_BMI\",\n",
    "    \"FFMI_BFP\",\n",
    "    \"FMI_BFP\",\n",
    "    \"LST_TBW\",\n",
    "    \"BFP_BMR\",\n",
    "    \"BFP_DEE\",\n",
    "    \"BMR_Weight\",\n",
    "    \"DEE_Weight\",\n",
    "    \"SMM_Height\",\n",
    "    \"Muscle_to_Fat\",\n",
    "    \"Hydration_Status\",\n",
    "    \"ICW_TBW\",\n",
    "]\n",
    "\n",
    "double_columns = [\n",
    "    \"FGC-FGC_SRR_Zone\",\n",
    "    \"BIA-BIA_SMM\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"BIA-BIA_FFMI\",\n",
    "    \"FGC-FGC_CU\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"BIA-BIA_ECW\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "    \"FGC-FGC_SRL_Zone\",\n",
    "    \"BIA-BIA_DEE\",\n",
    "    \"Physical-Weight\",\n",
    "    \"Fitness_Endurance-Time_Mins\",\n",
    "    \"FGC-FGC_SRR\",\n",
    "    \"SDS-SDS_Total_T\",\n",
    "    \"FGC-FGC_PU\",\n",
    "    \"BIA-BIA_FFM\",\n",
    "    \"FGC-FGC_TL_Zone\",\n",
    "    \"Physical-BMI\",\n",
    "    \"Physical-Systolic_BP\",\n",
    "    \"Physical-HeartRate\",\n",
    "    \"BIA-BIA_ICW\",\n",
    "    \"Physical-Height\",\n",
    "    \"FGC-FGC_SRL\",\n",
    "    \"BIA-BIA_BMC\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"BIA-BIA_Frame_num\",\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"FGC-FGC_GSND_Zone\",\n",
    "    \"Basic_Demos-Sex\",\n",
    "    \"FGC-FGC_GSND\",\n",
    "    \"BIA-BIA_LST\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"BIA-BIA_BMI\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"PAQ_C-PAQ_C_Total\",\n",
    "    \"BIA-BIA_Activity_Level_num\",\n",
    "    \"FGC-FGC_GSD\",\n",
    "    \"BIA-BIA_BMR\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"CGAS-CGAS_Score\",\n",
    "    \"FGC-FGC_PU_Zone\",\n",
    "    \"BIA-BIA_LDM\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"PAQ_A-PAQ_A_Total\",\n",
    "    \"BIA-BIA_TBW\",\n",
    "    \"FGC-FGC_GSD_Zone\",\n",
    "    \"Physical-Diastolic_BP\",\n",
    "]\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # season_cols = [col for col in df.columns if \"Season\" in col]\n",
    "    # df = df.drop(season_cols, axis=1)\n",
    "    df[\"BMI_Age\"] = df[\"Physical-BMI\"] * df[\"Basic_Demos-Age\"]\n",
    "    df[\"Internet_Hours_Age\"] = (\n",
    "        df[\"PreInt_EduHx-computerinternet_hoursday\"] * df[\"Basic_Demos-Age\"]\n",
    "    )\n",
    "    df[\"BMI_Internet_Hours\"] = (\n",
    "        df[\"Physical-BMI\"] * df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "    )\n",
    "    df[\"BFP_BMI\"] = df[\"BIA-BIA_Fat\"] / df[\"BIA-BIA_BMI\"]\n",
    "    df[\"FFMI_BFP\"] = df[\"BIA-BIA_FFMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"FMI_BFP\"] = df[\"BIA-BIA_FMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"LST_TBW\"] = df[\"BIA-BIA_LST\"] / df[\"BIA-BIA_TBW\"]\n",
    "    df[\"BFP_BMR\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_BMR\"]\n",
    "    df[\"BFP_DEE\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_DEE\"]\n",
    "    df[\"BMR_Weight\"] = df[\"BIA-BIA_BMR\"] / df[\"Physical-Weight\"]\n",
    "    df[\"DEE_Weight\"] = df[\"BIA-BIA_DEE\"] / df[\"Physical-Weight\"]\n",
    "    df[\"SMM_Height\"] = df[\"BIA-BIA_SMM\"] / df[\"Physical-Height\"]\n",
    "    df[\"Muscle_to_Fat\"] = df[\"BIA-BIA_SMM\"] / df[\"BIA-BIA_FMI\"]\n",
    "    df[\"Hydration_Status\"] = df[\"BIA-BIA_TBW\"] / df[\"Physical-Weight\"]\n",
    "    df[\"ICW_TBW\"] = df[\"BIA-BIA_ICW\"] / df[\"BIA-BIA_TBW\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "for add_ in add_features:\n",
    "    train[add_] = train[add_].fillna(0.0)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "\n",
    "test = feature_engineering(test)\n",
    "test = test.replace([np.inf, -np.inf], np.nan)\n",
    "for add_ in add_features:\n",
    "    test[add_] = test[add_].fillna(0.0)\n",
    "test = test.dropna(thresh=10, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31, 24, 60, 15) (31, 24, 60, 15)\n",
      "(31, 24, 15) (31, 24, 15) (31, 24, 30)\n",
      "(744, 30)\n",
      "torch.Size([1, 744, 30])\n",
      "CNN AutoEncoder output shape: torch.Size([1, 744, 30]) torch.Size([1, 128, 186])\n"
     ]
    }
   ],
   "source": [
    "# AutoEncoderの学習\n",
    "\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def read_parquet(base_dir, id_):\n",
    "    path = os.path.join(base_dir, f\"id={id_}\", \"part-0.parquet\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def get_valid_ids(base_dir):\n",
    "    return [f.split(\"=\")[1].split(\".\")[0] for f in os.listdir(base_dir)]\n",
    "\n",
    "\n",
    "# p = read_parquet(base_dir=\"../../../inputs/series_train.parquet/\", id_=\"ffcd4dbd\")\n",
    "# p = read_parquet(base_dir=\"../../inputs/series_train.parquet/\", id_=\"10e46254\")\n",
    "\n",
    "scale_columns = [\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"enmo\",\n",
    "    \"anglez\",\n",
    "    \"light\",\n",
    "    \"battery_voltage\",\n",
    "]\n",
    "\n",
    "masked_columns = [\n",
    "    \"masked_X\",\n",
    "    \"masked_Y\",\n",
    "    \"masked_Z\",\n",
    "    \"masked_enmo\",\n",
    "    \"masked_anglez\",\n",
    "    \"masked_light\",\n",
    "]\n",
    "\n",
    "original_columns = [\"battery_voltage\", \"non-wear_flag\"]\n",
    "\n",
    "p[\"non-wear_flag\"] = 1 - p[\"non-wear_flag\"]\n",
    "scaler_features = p[scale_columns].values\n",
    "scaler = StandardScaler()\n",
    "p[scale_columns] = scaler.fit_transform(scaler_features)\n",
    "\n",
    "for mask_col in masked_columns:\n",
    "    p[mask_col] = p[mask_col.replace(\"masked_\", \"\")] * p[\"non-wear_flag\"]\n",
    "\n",
    "p = p.fillna(0.0)\n",
    "\n",
    "groups = p.groupby(\"relative_date_PCIAT\")\n",
    "# グループごとにデータフレームのリストに分割\n",
    "chunks = [group.reset_index(drop=True) for _, group in groups]\n",
    "\n",
    "use_cols = masked_columns + original_columns + scale_columns\n",
    "watch_day = len(chunks)\n",
    "active_logs = np.zeros((31, 17280, len(use_cols)), dtype=np.float32)\n",
    "active_mask = np.zeros((31), dtype=np.int32)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i == 0:  #\n",
    "        active_logs[i, -len(chunk) :, :] = chunk[use_cols].values\n",
    "    elif i == watch_day:\n",
    "        active_logs[i, : len(chunk), :] = chunk[use_cols].values\n",
    "    else:\n",
    "        array = chunk[use_cols].values\n",
    "        active_logs[i, : len(array), :] = array\n",
    "\n",
    "    active_mask[i] = 1\n",
    "\n",
    "    if i == 30:\n",
    "        break\n",
    "\n",
    "active_logs = active_logs.reshape(31, 24, 60, 12, 15)  # 12は1時間の分割数\n",
    "active_logs_mean = active_logs.mean(axis=3)  # 1時間の分割数で平均を取る # 31, 1440, 15\n",
    "# active_logs_var = active_logs.var(axis=3)  # 1時間の分割数で分散を取る # 31, 1440, 15\n",
    "active_logs = np.concatenate([active_logs_mean], axis=-1)  # (31, 24, 30)\n",
    "# print(active_logs_mean.shape, active_logs_var.shape, active_logs.shape)\n",
    "\n",
    "print(active_logs_mean.shape, active_logs.shape)\n",
    "\n",
    "active_logs_mean = active_logs.mean(axis=2)  # 1時間の分割数で平均を取る # 31, 1440, 15\n",
    "active_logs_var = active_logs.var(axis=2)  # 1時間の分割数で分散を取る # 31, 1440, 15\n",
    "active_logs = np.concatenate(\n",
    "    [active_logs_mean, active_logs_var], axis=-1\n",
    ")  # (31, 24, 30)\n",
    "print(active_logs_mean.shape, active_logs_var.shape, active_logs.shape)\n",
    "active_logs = active_logs.reshape(-1, 30)\n",
    "print(active_logs.shape)\n",
    "\n",
    "# active_logs = active_logs.unsqueeze(0)\n",
    "active_logs = torch.tensor(active_logs, dtype=torch.float32).unsqueeze(0).to(\"cuda\")\n",
    "print(active_logs.shape)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CNNAutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNAutoEncoder, self).__init__()\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                in_channels=30, out_channels=64, kernel_size=3, stride=2, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose1d(\n",
    "                128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose1d(\n",
    "                64, 30, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # (batch, channel, time)\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.permute(0, 2, 1), encoded  # (batch, time, channel)\n",
    "\n",
    "\n",
    "# 実行例\n",
    "model = CNNAutoEncoder()\n",
    "input_data = torch.randn(1, 744, 30)\n",
    "output, embedding = model(input_data)\n",
    "print(\"CNN AutoEncoder output shape:\", output.shape, embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class CMIDataset(Dataset):\n",
    "    def __init__(self, table_df, valid_ids, base_dir, save_filename):\n",
    "        self.base_dir = base_dir\n",
    "        self.table_df = table_df\n",
    "        self.valid_ids = valid_ids\n",
    "        self.save_filename = save_filename\n",
    "        self.scale_columns = [\n",
    "            \"X\",\n",
    "            \"Y\",\n",
    "            \"Z\",\n",
    "            \"enmo\",\n",
    "            \"anglez\",\n",
    "            \"light\",\n",
    "            \"battery_voltage\",\n",
    "        ]\n",
    "\n",
    "        self.masked_columns = [\n",
    "            \"masked_X\",\n",
    "            \"masked_Y\",\n",
    "            \"masked_Z\",\n",
    "            \"masked_enmo\",\n",
    "            \"masked_anglez\",\n",
    "            \"masked_light\",\n",
    "        ]\n",
    "\n",
    "        self.original_columns = [\"battery_voltage\", \"non-wear_flag\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # テーブルデータの抽出\n",
    "        id_ = self.valid_ids[idx]\n",
    "\n",
    "        save_dir = f\"/home/tatsuya/code/projects/kaggle/ChildMindInstitute2024/precreated_dataset/{self.save_filename}/\"\n",
    "        save_path = os.path.join(save_dir, id_)\n",
    "\n",
    "        table = self.table_df.loc[self.table_df[\"id\"] == self.valid_ids[idx], :]\n",
    "        table_feature = table.drop(columns=[\"id\", \"sii\"]).values\n",
    "        sii = table[\"sii\"].values\n",
    "\n",
    "        # 時系列データの抽出\n",
    "        use_cols = self.masked_columns + self.original_columns + self.scale_columns\n",
    "        p = read_parquet(self.base_dir, self.valid_ids[idx])\n",
    "\n",
    "        if p is not None:\n",
    "            p[\"non-wear_flag\"] = 1 - p[\"non-wear_flag\"]\n",
    "            scaler_features = p[scale_columns].values\n",
    "            scaler = StandardScaler()\n",
    "            p[scale_columns] = scaler.fit_transform(scaler_features)\n",
    "\n",
    "            for mask_col in masked_columns:\n",
    "                p[mask_col] = p[mask_col.replace(\"masked_\", \"\")] * p[\"non-wear_flag\"]\n",
    "\n",
    "            p = p.fillna(0.0)\n",
    "\n",
    "            groups = p.groupby(\"relative_date_PCIAT\")\n",
    "            # グループごとにデータフレームのリストに分割\n",
    "            chunks = [group.reset_index(drop=True) for _, group in groups]\n",
    "\n",
    "            use_cols = masked_columns + original_columns + scale_columns\n",
    "            watch_day = len(chunks)\n",
    "            active_logs = np.zeros((31, 17280, len(use_cols)), dtype=np.float32)\n",
    "            active_mask = np.zeros((31), dtype=np.int32)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                if i == 0:  #\n",
    "                    active_logs[i, -len(chunk) :, :] = chunk[use_cols].values\n",
    "                elif i == watch_day:\n",
    "                    active_logs[i, : len(chunk), :] = chunk[use_cols].values\n",
    "                else:\n",
    "                    array = chunk[use_cols].values\n",
    "                    active_logs[i, : len(array), :] = array\n",
    "\n",
    "                active_mask[i] = 1\n",
    "\n",
    "                if i == 30:\n",
    "                    break\n",
    "\n",
    "            active_logs = active_logs.reshape(31, 24, 60, 12, 15)  # 12は1時間の分割数\n",
    "            active_logs_mean = active_logs.mean(\n",
    "                axis=3\n",
    "            )  # 1時間の分割数で平均を取る # 31, 1440, 15\n",
    "            # active_logs_var = active_logs.var(axis=3)  # 1時間の分割数で分散を取る # 31, 1440, 15\n",
    "            active_logs = np.concatenate([active_logs_mean], axis=-1)  # (31, 24, 30)\n",
    "            # print(active_logs_mean.shape, active_logs_var.shape, active_logs.shape)\n",
    "\n",
    "            # print(active_logs_mean.shape, active_logs.shape)\n",
    "\n",
    "            active_logs_mean = active_logs.mean(\n",
    "                axis=2\n",
    "            )  # 1時間の分割数で平均を取る # 31, 1440, 15\n",
    "            active_logs_var = active_logs.var(\n",
    "                axis=2\n",
    "            )  # 1時間の分割数で分散を取る # 31, 1440, 15\n",
    "            active_logs = np.concatenate(\n",
    "                [active_logs_mean, active_logs_var], axis=-1\n",
    "            )  # (31, 24, 30)\n",
    "            # print(active_logs_mean.shape, active_logs_var.shape, active_logs.shape)\n",
    "            active_logs = active_logs.reshape(-1, 30)\n",
    "\n",
    "        else:\n",
    "            active_logs = np.zeros((744, 30), dtype=np.float32)\n",
    "            active_mask = np.zeros((744), dtype=np.int32)\n",
    "\n",
    "        dataset_ = {\n",
    "            \"id\": id_,\n",
    "            # \"table_input\": torch.tensor(table_feature, dtype=torch.float32),\n",
    "            \"time_input\": torch.tensor(active_logs, dtype=torch.float32),\n",
    "            \"mask\": torch.tensor(active_mask, dtype=torch.int32),\n",
    "            \"output\": torch.tensor(sii, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "        return dataset_\n",
    "\n",
    "\n",
    "def read_parquet(base_dir, id_):\n",
    "    path = os.path.join(base_dir, f\"id={id_}\", \"part-0.parquet\")\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "dataset = CMIDataset(\n",
    "    table_df=train,\n",
    "    valid_ids=get_valid_ids(train_series_dir),\n",
    "    base_dir=train_series_dir,\n",
    "    save_filename=\"train\",\n",
    ")\n",
    "\n",
    "# AutoEncoderのモデルのインスタンス化\n",
    "cnn_model = CNNAutoEncoder().to(\"cuda\")\n",
    "cnn_model.load_state_dict(torch.load(\"./assets/cnn_autoencoder.pth\"))\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = torch.optim.Adam(cnn_model.parameters(), lr=0.0001)\n",
    "# # データセットからデータを取り出す\n",
    "\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# best_model = None\n",
    "# minimum_loss = 1000000\n",
    "\n",
    "# for epoch in range(10):\n",
    "#     print(f\"Epoch {epoch}\")\n",
    "#     dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "#     epoch_loss = []\n",
    "#     tq = tqdm(dataloader)\n",
    "#     for data in dataloader:\n",
    "#         optimizer.zero_grad()\n",
    "#         # table_input = data[\"table_input\"]\n",
    "#         time_input = data[\"time_input\"].to(\"cuda\")\n",
    "#         mask = data[\"mask\"]\n",
    "\n",
    "#         # モデルにデータを入力し、出力を取得\n",
    "#         cnn_output, embedding = cnn_model(time_input)\n",
    "#         # 損失の計算\n",
    "#         loss = criterion(cnn_output, time_input)\n",
    "#         loss.backward()\n",
    "\n",
    "#         optimizer.step()\n",
    "\n",
    "#         epoch_loss.append(loss.item())\n",
    "\n",
    "#         tq.set_postfix(loss=np.mean(epoch_loss))\n",
    "#         tq.update()\n",
    "\n",
    "#     if np.mean(epoch_loss) < minimum_loss:\n",
    "#         minimum_loss = np.mean(epoch_loss)\n",
    "#         best_model = cnn_model\n",
    "#         cnn_model.eval()\n",
    "#         torch.save(cnn_model.state_dict(), \"./assets/cnn_autoencoder.pth\")\n",
    "#         cnn_model.train()\n",
    "\n",
    "#     print(f\"Epoch {epoch} Loss: {np.mean(epoch_loss)}\")\n",
    "#     tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [01:41<00:00,  9.83it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = CMIDataset(\n",
    "    table_df=train,\n",
    "    valid_ids=get_valid_ids(train_series_dir),\n",
    "    base_dir=train_series_dir,\n",
    "    save_filename=\"train\",\n",
    ")\n",
    "\n",
    "# AutoEncoderのモデルのインスタンス化\n",
    "cnn_model = CNNAutoEncoder().to(\"cuda\")\n",
    "cnn_model.load_state_dict(torch.load(\"./assets/cnn_autoencoder.pth\"))\n",
    "# データセットからデータを取り出す\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_model = None\n",
    "minimum_loss = 1000000\n",
    "\n",
    "print(f\"Create Embedding\")\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "epoch_loss = []\n",
    "tq = tqdm(dataloader)\n",
    "\n",
    "embedding_result = []\n",
    "\n",
    "for data in dataloader:\n",
    "    id_ = data[\"id\"][0]\n",
    "    # table_input = data[\"table_input\"]\n",
    "    time_input = data[\"time_input\"].to(\"cuda\")\n",
    "    mask = data[\"mask\"]\n",
    "\n",
    "    # モデルにデータを入力し、出力を取得\n",
    "    cnn_output, embedding = cnn_model(time_input)\n",
    "    # 損失の計算\n",
    "\n",
    "    mean_embedding = embedding.squeeze(0).mean(axis=-1).cpu().detach().numpy()\n",
    "    # mean_embedding = embedding.cpu().detach().numpy()\n",
    "\n",
    "    embedding_result.append({\"id\": id_, \"embedding\": mean_embedding})\n",
    "\n",
    "    tq.update()\n",
    "\n",
    "tq.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>embedding_0</th>\n",
       "      <th>embedding_1</th>\n",
       "      <th>embedding_2</th>\n",
       "      <th>embedding_3</th>\n",
       "      <th>embedding_4</th>\n",
       "      <th>embedding_5</th>\n",
       "      <th>embedding_6</th>\n",
       "      <th>embedding_7</th>\n",
       "      <th>embedding_8</th>\n",
       "      <th>...</th>\n",
       "      <th>embedding_118</th>\n",
       "      <th>embedding_119</th>\n",
       "      <th>embedding_120</th>\n",
       "      <th>embedding_121</th>\n",
       "      <th>embedding_122</th>\n",
       "      <th>embedding_123</th>\n",
       "      <th>embedding_124</th>\n",
       "      <th>embedding_125</th>\n",
       "      <th>embedding_126</th>\n",
       "      <th>embedding_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23dafdab</td>\n",
       "      <td>0.934273</td>\n",
       "      <td>1.058295</td>\n",
       "      <td>1.028002</td>\n",
       "      <td>1.107266</td>\n",
       "      <td>1.511564</td>\n",
       "      <td>1.678031</td>\n",
       "      <td>0.614336</td>\n",
       "      <td>0.847055</td>\n",
       "      <td>0.827912</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049939</td>\n",
       "      <td>1.311329</td>\n",
       "      <td>0.996335</td>\n",
       "      <td>1.217725</td>\n",
       "      <td>0.973498</td>\n",
       "      <td>0.754627</td>\n",
       "      <td>1.093903</td>\n",
       "      <td>0.841220</td>\n",
       "      <td>1.063418</td>\n",
       "      <td>1.293948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e4614ec6</td>\n",
       "      <td>1.292569</td>\n",
       "      <td>1.470513</td>\n",
       "      <td>1.542059</td>\n",
       "      <td>1.298862</td>\n",
       "      <td>1.744134</td>\n",
       "      <td>1.957554</td>\n",
       "      <td>0.864446</td>\n",
       "      <td>1.181660</td>\n",
       "      <td>0.996835</td>\n",
       "      <td>...</td>\n",
       "      <td>1.476123</td>\n",
       "      <td>1.654581</td>\n",
       "      <td>1.056002</td>\n",
       "      <td>1.295083</td>\n",
       "      <td>1.551957</td>\n",
       "      <td>0.754289</td>\n",
       "      <td>1.351622</td>\n",
       "      <td>1.053791</td>\n",
       "      <td>1.531604</td>\n",
       "      <td>1.537732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56ef356c</td>\n",
       "      <td>1.030161</td>\n",
       "      <td>1.136603</td>\n",
       "      <td>1.320513</td>\n",
       "      <td>1.339276</td>\n",
       "      <td>1.893055</td>\n",
       "      <td>1.956167</td>\n",
       "      <td>0.807934</td>\n",
       "      <td>1.109800</td>\n",
       "      <td>1.112787</td>\n",
       "      <td>...</td>\n",
       "      <td>1.311299</td>\n",
       "      <td>1.835630</td>\n",
       "      <td>1.093886</td>\n",
       "      <td>1.421084</td>\n",
       "      <td>1.230976</td>\n",
       "      <td>0.685264</td>\n",
       "      <td>1.468262</td>\n",
       "      <td>1.255411</td>\n",
       "      <td>1.170305</td>\n",
       "      <td>1.344505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dcfcd574</td>\n",
       "      <td>1.437756</td>\n",
       "      <td>1.521565</td>\n",
       "      <td>1.574320</td>\n",
       "      <td>1.582690</td>\n",
       "      <td>1.890268</td>\n",
       "      <td>2.237839</td>\n",
       "      <td>0.957044</td>\n",
       "      <td>1.491451</td>\n",
       "      <td>1.149451</td>\n",
       "      <td>...</td>\n",
       "      <td>1.768229</td>\n",
       "      <td>1.925022</td>\n",
       "      <td>1.165750</td>\n",
       "      <td>1.321340</td>\n",
       "      <td>1.601160</td>\n",
       "      <td>0.861113</td>\n",
       "      <td>1.609951</td>\n",
       "      <td>1.270303</td>\n",
       "      <td>1.577081</td>\n",
       "      <td>1.733535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>338146bd</td>\n",
       "      <td>0.833982</td>\n",
       "      <td>0.835131</td>\n",
       "      <td>1.005347</td>\n",
       "      <td>1.011853</td>\n",
       "      <td>1.326406</td>\n",
       "      <td>1.422274</td>\n",
       "      <td>0.602682</td>\n",
       "      <td>0.865930</td>\n",
       "      <td>0.832643</td>\n",
       "      <td>...</td>\n",
       "      <td>0.994025</td>\n",
       "      <td>1.280672</td>\n",
       "      <td>0.815593</td>\n",
       "      <td>0.957112</td>\n",
       "      <td>0.948529</td>\n",
       "      <td>0.525334</td>\n",
       "      <td>1.072017</td>\n",
       "      <td>0.860638</td>\n",
       "      <td>0.921265</td>\n",
       "      <td>1.005224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2a9e0dee</td>\n",
       "      <td>1.118476</td>\n",
       "      <td>1.154102</td>\n",
       "      <td>1.328735</td>\n",
       "      <td>1.243005</td>\n",
       "      <td>1.651012</td>\n",
       "      <td>1.804135</td>\n",
       "      <td>0.703245</td>\n",
       "      <td>1.077119</td>\n",
       "      <td>0.960325</td>\n",
       "      <td>...</td>\n",
       "      <td>1.295312</td>\n",
       "      <td>1.530142</td>\n",
       "      <td>1.013229</td>\n",
       "      <td>1.196613</td>\n",
       "      <td>1.293185</td>\n",
       "      <td>0.606831</td>\n",
       "      <td>1.258096</td>\n",
       "      <td>0.982478</td>\n",
       "      <td>1.243394</td>\n",
       "      <td>1.284215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0eddd8e5</td>\n",
       "      <td>1.003418</td>\n",
       "      <td>1.115285</td>\n",
       "      <td>1.469960</td>\n",
       "      <td>1.305835</td>\n",
       "      <td>2.142858</td>\n",
       "      <td>2.006252</td>\n",
       "      <td>0.750761</td>\n",
       "      <td>1.160900</td>\n",
       "      <td>1.059262</td>\n",
       "      <td>...</td>\n",
       "      <td>1.395912</td>\n",
       "      <td>1.999641</td>\n",
       "      <td>1.101297</td>\n",
       "      <td>1.405424</td>\n",
       "      <td>1.463978</td>\n",
       "      <td>0.790238</td>\n",
       "      <td>1.587342</td>\n",
       "      <td>1.133078</td>\n",
       "      <td>1.314270</td>\n",
       "      <td>1.498254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a49eda7f</td>\n",
       "      <td>1.112953</td>\n",
       "      <td>1.273634</td>\n",
       "      <td>1.532017</td>\n",
       "      <td>1.440813</td>\n",
       "      <td>1.984239</td>\n",
       "      <td>1.999086</td>\n",
       "      <td>0.782063</td>\n",
       "      <td>1.181599</td>\n",
       "      <td>1.059615</td>\n",
       "      <td>...</td>\n",
       "      <td>1.443727</td>\n",
       "      <td>1.882844</td>\n",
       "      <td>1.108292</td>\n",
       "      <td>1.392762</td>\n",
       "      <td>1.356991</td>\n",
       "      <td>0.621868</td>\n",
       "      <td>1.475014</td>\n",
       "      <td>1.189176</td>\n",
       "      <td>1.345740</td>\n",
       "      <td>1.492343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fa34f945</td>\n",
       "      <td>0.885967</td>\n",
       "      <td>0.950511</td>\n",
       "      <td>0.895033</td>\n",
       "      <td>1.111232</td>\n",
       "      <td>1.495834</td>\n",
       "      <td>1.656162</td>\n",
       "      <td>0.655794</td>\n",
       "      <td>0.761335</td>\n",
       "      <td>0.848918</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853265</td>\n",
       "      <td>1.006434</td>\n",
       "      <td>0.973718</td>\n",
       "      <td>1.090717</td>\n",
       "      <td>0.844414</td>\n",
       "      <td>0.705840</td>\n",
       "      <td>1.027594</td>\n",
       "      <td>0.745894</td>\n",
       "      <td>0.964948</td>\n",
       "      <td>1.118776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>526f719b</td>\n",
       "      <td>0.957307</td>\n",
       "      <td>0.889254</td>\n",
       "      <td>1.175763</td>\n",
       "      <td>1.011990</td>\n",
       "      <td>1.275229</td>\n",
       "      <td>1.539330</td>\n",
       "      <td>0.532177</td>\n",
       "      <td>0.921299</td>\n",
       "      <td>0.885462</td>\n",
       "      <td>...</td>\n",
       "      <td>1.122689</td>\n",
       "      <td>1.329937</td>\n",
       "      <td>0.939272</td>\n",
       "      <td>0.975884</td>\n",
       "      <td>1.162482</td>\n",
       "      <td>0.528119</td>\n",
       "      <td>1.144915</td>\n",
       "      <td>0.932701</td>\n",
       "      <td>1.067682</td>\n",
       "      <td>1.111611</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>996 rows × 129 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  embedding_0  embedding_1  embedding_2  embedding_3  embedding_4  \\\n",
       "0   23dafdab     0.934273     1.058295     1.028002     1.107266     1.511564   \n",
       "0   e4614ec6     1.292569     1.470513     1.542059     1.298862     1.744134   \n",
       "0   56ef356c     1.030161     1.136603     1.320513     1.339276     1.893055   \n",
       "0   dcfcd574     1.437756     1.521565     1.574320     1.582690     1.890268   \n",
       "0   338146bd     0.833982     0.835131     1.005347     1.011853     1.326406   \n",
       "..       ...          ...          ...          ...          ...          ...   \n",
       "0   2a9e0dee     1.118476     1.154102     1.328735     1.243005     1.651012   \n",
       "0   0eddd8e5     1.003418     1.115285     1.469960     1.305835     2.142858   \n",
       "0   a49eda7f     1.112953     1.273634     1.532017     1.440813     1.984239   \n",
       "0   fa34f945     0.885967     0.950511     0.895033     1.111232     1.495834   \n",
       "0   526f719b     0.957307     0.889254     1.175763     1.011990     1.275229   \n",
       "\n",
       "    embedding_5  embedding_6  embedding_7  embedding_8  ...  embedding_118  \\\n",
       "0      1.678031     0.614336     0.847055     0.827912  ...       1.049939   \n",
       "0      1.957554     0.864446     1.181660     0.996835  ...       1.476123   \n",
       "0      1.956167     0.807934     1.109800     1.112787  ...       1.311299   \n",
       "0      2.237839     0.957044     1.491451     1.149451  ...       1.768229   \n",
       "0      1.422274     0.602682     0.865930     0.832643  ...       0.994025   \n",
       "..          ...          ...          ...          ...  ...            ...   \n",
       "0      1.804135     0.703245     1.077119     0.960325  ...       1.295312   \n",
       "0      2.006252     0.750761     1.160900     1.059262  ...       1.395912   \n",
       "0      1.999086     0.782063     1.181599     1.059615  ...       1.443727   \n",
       "0      1.656162     0.655794     0.761335     0.848918  ...       0.853265   \n",
       "0      1.539330     0.532177     0.921299     0.885462  ...       1.122689   \n",
       "\n",
       "    embedding_119  embedding_120  embedding_121  embedding_122  embedding_123  \\\n",
       "0        1.311329       0.996335       1.217725       0.973498       0.754627   \n",
       "0        1.654581       1.056002       1.295083       1.551957       0.754289   \n",
       "0        1.835630       1.093886       1.421084       1.230976       0.685264   \n",
       "0        1.925022       1.165750       1.321340       1.601160       0.861113   \n",
       "0        1.280672       0.815593       0.957112       0.948529       0.525334   \n",
       "..            ...            ...            ...            ...            ...   \n",
       "0        1.530142       1.013229       1.196613       1.293185       0.606831   \n",
       "0        1.999641       1.101297       1.405424       1.463978       0.790238   \n",
       "0        1.882844       1.108292       1.392762       1.356991       0.621868   \n",
       "0        1.006434       0.973718       1.090717       0.844414       0.705840   \n",
       "0        1.329937       0.939272       0.975884       1.162482       0.528119   \n",
       "\n",
       "    embedding_124  embedding_125  embedding_126  embedding_127  \n",
       "0        1.093903       0.841220       1.063418       1.293948  \n",
       "0        1.351622       1.053791       1.531604       1.537732  \n",
       "0        1.468262       1.255411       1.170305       1.344505  \n",
       "0        1.609951       1.270303       1.577081       1.733535  \n",
       "0        1.072017       0.860638       0.921265       1.005224  \n",
       "..            ...            ...            ...            ...  \n",
       "0        1.258096       0.982478       1.243394       1.284215  \n",
       "0        1.587342       1.133078       1.314270       1.498254  \n",
       "0        1.475014       1.189176       1.345740       1.492343  \n",
       "0        1.027594       0.745894       0.964948       1.118776  \n",
       "0        1.144915       0.932701       1.067682       1.111611  \n",
       "\n",
       "[996 rows x 129 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_df_all = None\n",
    "\n",
    "for row in embedding_result:\n",
    "    id_ = row[\"id\"]\n",
    "    embedding = row[\"embedding\"]\n",
    "    embedding_cols = [f\"embedding_{i}\" for i in range(embedding.shape[-1])]\n",
    "    embedding_df = pd.DataFrame(embedding.reshape(1, -1), columns=embedding_cols)\n",
    "    embedding_df[\"id\"] = id_\n",
    "\n",
    "    if embedding_df_all is None:\n",
    "        embedding_df_all = embedding_df\n",
    "    else:\n",
    "        embedding_df_all = pd.concat([embedding_df_all, embedding_df], axis=0)\n",
    "\n",
    "embedding_df_all = embedding_df_all[[\"id\"] + embedding_cols]\n",
    "embedding_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.merge(embedding_df_all, on=\"id\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 - Train QWK: 0.7796, Validation QWK: 0.3546\n",
      "Fold 2 - Train QWK: 0.7592, Validation QWK: 0.4247\n",
      "Fold 3 - Train QWK: 0.7693, Validation QWK: 0.4009\n",
      "Fold 4 - Train QWK: 0.7504, Validation QWK: 0.4053\n",
      "Fold 5 - Train QWK: 0.7645, Validation QWK: 0.4264\n",
      "CV: 0.4024\n",
      "tuned Kappa: 0.449\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import (\n",
    "    VotingRegressor,\n",
    "    RandomForestRegressor,\n",
    "    GradientBoostingRegressor,\n",
    ")\n",
    "\n",
    "\n",
    "def extract_features(df):\n",
    "    return df[double_columns + add_features + embedding_cols]\n",
    "\n",
    "\n",
    "seed = 42\n",
    "oof = []\n",
    "cv_scores = []\n",
    "y = None\n",
    "\n",
    "# Model parameters for LightGBM\n",
    "Params = {\n",
    "    \"learning_rate\": 0.046,\n",
    "    \"max_depth\": 12,\n",
    "    \"num_leaves\": 478,\n",
    "    \"min_data_in_leaf\": 13,\n",
    "    \"feature_fraction\": 0.893,\n",
    "    \"bagging_fraction\": 0.784,\n",
    "    \"bagging_freq\": 4,\n",
    "    \"lambda_l1\": 10,  # Increased from 6.59\n",
    "    \"lambda_l2\": 0.01,  # Increased from 2.68e-06\n",
    "}\n",
    "XGB_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"max_depth\": 6,\n",
    "    \"n_estimators\": 200,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"reg_alpha\": 1,  # Increased from 0.1\n",
    "    \"reg_lambda\": 5,  # Increased from 1\n",
    "    \"random_state\": seed,\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 6,\n",
    "    \"iterations\": 200,\n",
    "    \"random_seed\": seed,\n",
    "    \"verbose\": 0,\n",
    "    \"l2_leaf_reg\": 10,  # Increase this value\n",
    "}\n",
    "\n",
    "for fold in range(5):\n",
    "    Light = LGBMRegressor(**Params, random_state=seed, verbose=-1, n_estimators=300)\n",
    "    XGB_Model = XGBRegressor(**XGB_Params)\n",
    "    CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "    model = VotingRegressor(\n",
    "        estimators=[\n",
    "            (\"lightgbm\", Light),\n",
    "            (\"xgboost\", XGB_Model),\n",
    "            (\"catboost\", CatBoost_Model),\n",
    "            # ('tabnet', TabNet_Model)\n",
    "        ]\n",
    "    )\n",
    "    # model = LGBMRegressor(**Params, random_state=seed, verbose=-1, n_estimators=300)\n",
    "\n",
    "    with open(f\"../divided-datasets/fold_train_ids_{fold}.pkl\", \"rb\") as f:\n",
    "        fold_train_ids = pickle.load(f)\n",
    "\n",
    "    with open(f\"../divided-datasets/fold_valid_ids_{fold}.pkl\", \"rb\") as f:\n",
    "        fold_valid_ids = pickle.load(f)\n",
    "\n",
    "    train_fold = train[train[\"id\"].isin(fold_train_ids)].reset_index(drop=True)\n",
    "    valid_fold = train[train[\"id\"].isin(fold_valid_ids)].reset_index(drop=True)\n",
    "\n",
    "    mode = \"drop\"\n",
    "\n",
    "    if mode == \"impute\":\n",
    "        numeric_cols = train.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        train_fold[numeric_cols] = imputer.fit_transform(train_fold[numeric_cols])\n",
    "        test[numeric_cols] = imputer.transform(test[numeric_cols])\n",
    "    elif mode == \"drop\":\n",
    "        train_fold = train_fold[train_fold[\"sii\"].notnull()].reset_index(drop=True)\n",
    "\n",
    "    train_fold_x = extract_features(train_fold)\n",
    "    valid_fold_x = extract_features(valid_fold)\n",
    "\n",
    "    train_fold_y = train_fold[\"sii\"].astype(int)\n",
    "    valid_fold_y = valid_fold[\"sii\"].astype(int)\n",
    "\n",
    "    model.fit(train_fold_x, train_fold_y)\n",
    "\n",
    "    # save model\n",
    "    with open(f\"./assets/model02_{fold}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    train_pred = model.predict(train_fold_x)\n",
    "    valid_pred = model.predict(valid_fold_x)\n",
    "\n",
    "    train_kappa = quadratic_weighted_kappa(\n",
    "        train_fold_y, train_pred.round(0).astype(int)\n",
    "    )\n",
    "    val_kappa = quadratic_weighted_kappa(valid_fold_y, valid_pred.round(0).astype(int))\n",
    "    cv_scores.append(val_kappa)\n",
    "\n",
    "    print(\n",
    "        f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\"\n",
    "    )\n",
    "\n",
    "    for i, id_ in enumerate(fold_valid_ids):\n",
    "        oof.append({\"id\": id_, \"sii\": valid_pred[i]})\n",
    "\n",
    "    if y is None:\n",
    "        y = valid_fold[[\"id\", \"sii\"]]\n",
    "    else:\n",
    "        y = pd.concat([y, valid_fold[[\"id\", \"sii\"]]], axis=0).reset_index(drop=True)\n",
    "\n",
    "oof = pd.DataFrame(oof)\n",
    "\n",
    "KappaOPtimizer = minimize(\n",
    "    evaluate_predictions,\n",
    "    x0=[0.5, 1.5, 2.5],\n",
    "    args=(y[\"sii\"].astype(int), oof[\"sii\"]),\n",
    "    method=\"Nelder-Mead\",\n",
    ")\n",
    "\n",
    "oof_tuned = threshold_Rounder(oof[\"sii\"], KappaOPtimizer.x)\n",
    "tKappa = quadratic_weighted_kappa(y[\"sii\"], oof_tuned)\n",
    "print(f\"CV: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"tuned Kappa: {tKappa:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof.to_csv(\"./oof/oof.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
