{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from src.dataloader_ import *\n",
    "from src.network_ import *\n",
    "from src.utils import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_series_dir = \"../../inputs/series_train.parquet/\"\n",
    "test_series_dir = \"../../inputs/series_test.parquet/\"\n",
    "\n",
    "data_dic_path = \"../../inputs/data_dictionary.csv\"\n",
    "sample_submission_path = \"../../inputs/sample_submission.csv\"\n",
    "train_path = \"../../inputs/train.csv\"\n",
    "test_path = \"../../inputs/test.csv\"\n",
    "\n",
    "train = pd.read_csv(train_path)\n",
    "test = pd.read_csv(test_path)\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "data_dic = pd.read_csv(data_dic_path)\n",
    "\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def seed_torch(seed=1029):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "nb_name = os.path.basename(os.getcwd())  # notebook name\n",
    "seed_torch(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "def feature_engineering(df):\n",
    "    # season_cols = [col for col in df.columns if \"Season\" in col]\n",
    "    # df = df.drop(season_cols, axis=1)\n",
    "    df[\"BMI_Age\"] = df[\"Physical-BMI\"] * df[\"Basic_Demos-Age\"]\n",
    "    df[\"Internet_Hours_Age\"] = (\n",
    "        df[\"PreInt_EduHx-computerinternet_hoursday\"] * df[\"Basic_Demos-Age\"]\n",
    "    )\n",
    "    df[\"BMI_Internet_Hours\"] = (\n",
    "        df[\"Physical-BMI\"] * df[\"PreInt_EduHx-computerinternet_hoursday\"]\n",
    "    )\n",
    "    df[\"BFP_BMI\"] = df[\"BIA-BIA_Fat\"] / df[\"BIA-BIA_BMI\"]\n",
    "    df[\"FFMI_BFP\"] = df[\"BIA-BIA_FFMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"FMI_BFP\"] = df[\"BIA-BIA_FMI\"] / df[\"BIA-BIA_Fat\"]\n",
    "    df[\"LST_TBW\"] = df[\"BIA-BIA_LST\"] / df[\"BIA-BIA_TBW\"]\n",
    "    df[\"BFP_BMR\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_BMR\"]\n",
    "    df[\"BFP_DEE\"] = df[\"BIA-BIA_Fat\"] * df[\"BIA-BIA_DEE\"]\n",
    "    df[\"BMR_Weight\"] = df[\"BIA-BIA_BMR\"] / df[\"Physical-Weight\"]\n",
    "    df[\"DEE_Weight\"] = df[\"BIA-BIA_DEE\"] / df[\"Physical-Weight\"]\n",
    "    df[\"SMM_Height\"] = df[\"BIA-BIA_SMM\"] / df[\"Physical-Height\"]\n",
    "    df[\"Muscle_to_Fat\"] = df[\"BIA-BIA_SMM\"] / df[\"BIA-BIA_FMI\"]\n",
    "    df[\"Hydration_Status\"] = df[\"BIA-BIA_TBW\"] / df[\"Physical-Weight\"]\n",
    "    df[\"ICW_TBW\"] = df[\"BIA-BIA_ICW\"] / df[\"BIA-BIA_TBW\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = test.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "# train_imputed[\"sii\"] = train_imputed[\"sii\"].round().astype(int)\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train[col]\n",
    "\n",
    "train = train_imputed\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "train_id_df = train[\"id\"]\n",
    "test_id_df = test[\"id\"]\n",
    "\n",
    "train = train.drop(\"id\", axis=1)\n",
    "test = test.drop(\"id\", axis=1)\n",
    "\n",
    "\n",
    "featuresCols = [\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"Basic_Demos-Sex\",\n",
    "    \"CGAS-CGAS_Score\",\n",
    "    \"Physical-BMI\",\n",
    "    \"Physical-Height\",\n",
    "    \"Physical-Weight\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"Physical-Diastolic_BP\",\n",
    "    \"Physical-HeartRate\",\n",
    "    \"Physical-Systolic_BP\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"Fitness_Endurance-Time_Mins\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"FGC-FGC_CU\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "    \"FGC-FGC_GSND\",\n",
    "    \"FGC-FGC_GSND_Zone\",\n",
    "    \"FGC-FGC_GSD\",\n",
    "    \"FGC-FGC_GSD_Zone\",\n",
    "    \"FGC-FGC_PU\",\n",
    "    \"FGC-FGC_PU_Zone\",\n",
    "    \"FGC-FGC_SRL\",\n",
    "    \"FGC-FGC_SRL_Zone\",\n",
    "    \"FGC-FGC_SRR\",\n",
    "    \"FGC-FGC_SRR_Zone\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"FGC-FGC_TL_Zone\",\n",
    "    \"BIA-BIA_Activity_Level_num\",\n",
    "    \"BIA-BIA_BMC\",\n",
    "    \"BIA-BIA_BMI\",\n",
    "    \"BIA-BIA_BMR\",\n",
    "    \"BIA-BIA_DEE\",\n",
    "    \"BIA-BIA_ECW\",\n",
    "    \"BIA-BIA_FFM\",\n",
    "    \"BIA-BIA_FFMI\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"BIA-BIA_Frame_num\",\n",
    "    \"BIA-BIA_ICW\",\n",
    "    \"BIA-BIA_LDM\",\n",
    "    \"BIA-BIA_LST\",\n",
    "    \"BIA-BIA_SMM\",\n",
    "    \"BIA-BIA_TBW\",\n",
    "    \"PAQ_A-PAQ_A_Total\",\n",
    "    \"PAQ_C-PAQ_C_Total\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"SDS-SDS_Total_T\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"sii\",\n",
    "    \"BMI_Age\",\n",
    "    \"Internet_Hours_Age\",\n",
    "    \"BMI_Internet_Hours\",\n",
    "    \"BFP_BMI\",\n",
    "    \"FFMI_BFP\",\n",
    "    \"FMI_BFP\",\n",
    "    \"LST_TBW\",\n",
    "    \"BFP_BMR\",\n",
    "    \"BFP_DEE\",\n",
    "    \"BMR_Weight\",\n",
    "    \"DEE_Weight\",\n",
    "    \"SMM_Height\",\n",
    "    \"Muscle_to_Fat\",\n",
    "    \"Hydration_Status\",\n",
    "    \"ICW_TBW\",\n",
    "]\n",
    "\n",
    "# featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset=\"sii\")\n",
    "train_sii_df = train[\"sii\"]\n",
    "train = train.drop(\"sii\", axis=1)\n",
    "\n",
    "featuresCols = [\n",
    "    \"Basic_Demos-Age\",\n",
    "    \"Basic_Demos-Sex\",\n",
    "    \"CGAS-CGAS_Score\",\n",
    "    \"Physical-BMI\",\n",
    "    \"Physical-Height\",\n",
    "    \"Physical-Weight\",\n",
    "    \"Physical-Waist_Circumference\",\n",
    "    \"Physical-Diastolic_BP\",\n",
    "    \"Physical-HeartRate\",\n",
    "    \"Physical-Systolic_BP\",\n",
    "    \"Fitness_Endurance-Max_Stage\",\n",
    "    \"Fitness_Endurance-Time_Mins\",\n",
    "    \"Fitness_Endurance-Time_Sec\",\n",
    "    \"FGC-FGC_CU\",\n",
    "    \"FGC-FGC_CU_Zone\",\n",
    "    \"FGC-FGC_GSND\",\n",
    "    \"FGC-FGC_GSND_Zone\",\n",
    "    \"FGC-FGC_GSD\",\n",
    "    \"FGC-FGC_GSD_Zone\",\n",
    "    \"FGC-FGC_PU\",\n",
    "    \"FGC-FGC_PU_Zone\",\n",
    "    \"FGC-FGC_SRL\",\n",
    "    \"FGC-FGC_SRL_Zone\",\n",
    "    \"FGC-FGC_SRR\",\n",
    "    \"FGC-FGC_SRR_Zone\",\n",
    "    \"FGC-FGC_TL\",\n",
    "    \"FGC-FGC_TL_Zone\",\n",
    "    \"BIA-BIA_Activity_Level_num\",\n",
    "    \"BIA-BIA_BMC\",\n",
    "    \"BIA-BIA_BMI\",\n",
    "    \"BIA-BIA_BMR\",\n",
    "    \"BIA-BIA_DEE\",\n",
    "    \"BIA-BIA_ECW\",\n",
    "    \"BIA-BIA_FFM\",\n",
    "    \"BIA-BIA_FFMI\",\n",
    "    \"BIA-BIA_FMI\",\n",
    "    \"BIA-BIA_Fat\",\n",
    "    \"BIA-BIA_Frame_num\",\n",
    "    \"BIA-BIA_ICW\",\n",
    "    \"BIA-BIA_LDM\",\n",
    "    \"BIA-BIA_LST\",\n",
    "    \"BIA-BIA_SMM\",\n",
    "    \"BIA-BIA_TBW\",\n",
    "    \"PAQ_A-PAQ_A_Total\",\n",
    "    \"PAQ_C-PAQ_C_Total\",\n",
    "    \"SDS-SDS_Total_Raw\",\n",
    "    \"SDS-SDS_Total_T\",\n",
    "    \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "    \"BMI_Age\",\n",
    "    \"Internet_Hours_Age\",\n",
    "    \"BMI_Internet_Hours\",\n",
    "    \"BFP_BMI\",\n",
    "    \"FFMI_BFP\",\n",
    "    \"FMI_BFP\",\n",
    "    \"LST_TBW\",\n",
    "    \"BFP_BMR\",\n",
    "    \"BFP_DEE\",\n",
    "    \"BMR_Weight\",\n",
    "    \"DEE_Weight\",\n",
    "    \"SMM_Height\",\n",
    "    \"Muscle_to_Fat\",\n",
    "    \"Hydration_Status\",\n",
    "    \"ICW_TBW\",\n",
    "]\n",
    "\n",
    "# featuresCols += time_series_cols\n",
    "test = test[featuresCols]\n",
    "test[\"id\"] = test_id_df\n",
    "\n",
    "# column名をfeature_iに変更\n",
    "\n",
    "feature_cols = [f\"feature_{i}\" for i in range(train.shape[1])]\n",
    "train.columns = feature_cols\n",
    "train[\"id\"] = train_id_df\n",
    "train[\"sii\"] = train_sii_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# scaler = MinMaxScaler()\n",
    "\n",
    "# 各列のinf, -infを各列における最大値、最小値に変換\n",
    "train = train.replace([np.inf, -np.inf], np.nan)\n",
    "train = train.fillna(train.max())\n",
    "\n",
    "train[feature_cols] = scaler.fit_transform(train[feature_cols].values)\n",
    "\n",
    "with open(\"./assets/scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### テーブルデータセット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # onehotEncoderの作成\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# categorical_columns = [\n",
    "#     \"Basic_Demos-Enroll_Season\",\n",
    "#     \"CGAS-Season\",\n",
    "#     \"Physical-Season\",\n",
    "#     \"PAQ_C-Season\",\n",
    "#     \"FGC-Season\",\n",
    "#     \"Fitness_Endurance-Season\",\n",
    "#     \"PAQ_A-Season\",\n",
    "#     \"BIA-Season\",\n",
    "#     \"SDS-Season\",\n",
    "#     \"PreInt_EduHx-Season\",\n",
    "# ]\n",
    "\n",
    "# double_columns = [\n",
    "#     \"FGC-FGC_SRR_Zone\",\n",
    "#     \"BIA-BIA_SMM\",\n",
    "#     \"Physical-Waist_Circumference\",\n",
    "#     \"BIA-BIA_FFMI\",\n",
    "#     \"FGC-FGC_CU\",\n",
    "#     \"PreInt_EduHx-computerinternet_hoursday\",\n",
    "#     \"BIA-BIA_ECW\",\n",
    "#     \"FGC-FGC_CU_Zone\",\n",
    "#     \"FGC-FGC_SRL_Zone\",\n",
    "#     \"BIA-BIA_DEE\",\n",
    "#     \"Physical-Weight\",\n",
    "#     \"Fitness_Endurance-Time_Mins\",\n",
    "#     \"FGC-FGC_SRR\",\n",
    "#     \"SDS-SDS_Total_T\",\n",
    "#     \"FGC-FGC_PU\",\n",
    "#     \"BIA-BIA_FFM\",\n",
    "#     \"FGC-FGC_TL_Zone\",\n",
    "#     \"Physical-BMI\",\n",
    "#     \"Physical-Systolic_BP\",\n",
    "#     \"Physical-HeartRate\",\n",
    "#     \"BIA-BIA_ICW\",\n",
    "#     \"Physical-Height\",\n",
    "#     \"FGC-FGC_SRL\",\n",
    "#     \"BIA-BIA_BMC\",\n",
    "#     \"Fitness_Endurance-Time_Sec\",\n",
    "#     \"BIA-BIA_Frame_num\",\n",
    "#     \"Basic_Demos-Age\",\n",
    "#     \"FGC-FGC_GSND_Zone\",\n",
    "#     \"Basic_Demos-Sex\",\n",
    "#     \"FGC-FGC_GSND\",\n",
    "#     \"BIA-BIA_LST\",\n",
    "#     \"FGC-FGC_TL\",\n",
    "#     \"BIA-BIA_BMI\",\n",
    "#     \"BIA-BIA_FMI\",\n",
    "#     \"PAQ_C-PAQ_C_Total\",\n",
    "#     \"BIA-BIA_Activity_Level_num\",\n",
    "#     \"FGC-FGC_GSD\",\n",
    "#     \"BIA-BIA_BMR\",\n",
    "#     \"BIA-BIA_Fat\",\n",
    "#     \"SDS-SDS_Total_Raw\",\n",
    "#     \"CGAS-CGAS_Score\",\n",
    "#     \"FGC-FGC_PU_Zone\",\n",
    "#     \"BIA-BIA_LDM\",\n",
    "#     \"Fitness_Endurance-Max_Stage\",\n",
    "#     \"PAQ_A-PAQ_A_Total\",\n",
    "#     \"BIA-BIA_TBW\",\n",
    "#     \"FGC-FGC_GSD_Zone\",\n",
    "#     \"Physical-Diastolic_BP\",\n",
    "# ]\n",
    "\n",
    "# ###################### categorical columns ######################\n",
    "# # trainのtargetをonehot化\n",
    "# onehot_encoder = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "# onehot_encoder.fit(train[categorical_columns])\n",
    "\n",
    "# with open(\"./assets/onehot_encoder.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(onehot_encoder, f)\n",
    "\n",
    "# categorical_feature = onehot_encoder.transform(train[categorical_columns])\n",
    "\n",
    "# ###################### double columns ######################\n",
    "# # trainのtargetを標準化\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(train[double_columns + add_features])\n",
    "\n",
    "# with open(\"./assets/scaler.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(scaler, f)\n",
    "\n",
    "# double_feature = scaler.transform(train[double_columns + add_features])\n",
    "# # double_feature = train[double_columns].values\n",
    "\n",
    "# # 欠損値の補完\n",
    "# double_feature = np.nan_to_num(double_feature)\n",
    "\n",
    "# ###################### inputの作成 ######################\n",
    "\n",
    "# ids = train[\"id\"].values.reshape(-1, 1)\n",
    "# X = np.concatenate([categorical_feature, double_feature], axis=1)\n",
    "# y = train[\"sii\"].fillna(-1).values.reshape(-1, 1)\n",
    "\n",
    "# # DataFrameの作成\n",
    "# ids_df = pd.DataFrame(ids, columns=[\"id\"])\n",
    "# X_df = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "# y_df = pd.DataFrame(y, columns=[\"sii\"])\n",
    "\n",
    "# train_df = pd.concat([ids_df, X_df, y_df], axis=1)\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet(base_dir, id_):\n",
    "    path = os.path.join(base_dir, f\"id={id_}\", \"part-0.parquet\")\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def get_valid_ids(base_dir):\n",
    "    return [f.split(\"=\")[1].split(\".\")[0] for f in os.listdir(base_dir)]\n",
    "\n",
    "\n",
    "p = read_parquet(base_dir=\"../../inputs/series_train.parquet/\", id_=\"ffcd4dbd\")\n",
    "# p = read_parquet(base_dir=\"../../inputs/series_train.parquet/\", id_=\"10e46254\")\n",
    "# p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "# len(glob(\"../../normalized/*\"))\n",
    "len(glob(\"../../inputs/series_train.parquet/*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n",
    "\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(\n",
    "        oof_non_rounded < thresholds[0],\n",
    "        0,\n",
    "        np.where(\n",
    "            oof_non_rounded < thresholds[1],\n",
    "            1,\n",
    "            np.where(oof_non_rounded < thresholds[2], 2, 3),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "use_ids = list(\n",
    "    train_df[train_df[\"sii\"] != -1][\"id\"].unique()\n",
    ")  # get_valid_ids(base_dir=\"../../normalized/\")\n",
    "\n",
    "len(use_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "use_ids = np.array(use_ids)\n",
    "for train_index, valid_index in kf.split(use_ids):\n",
    "    train_ids = [use_ids[i] for i in train_index]\n",
    "    valid_ids = [use_ids[i] for i in valid_index]\n",
    "\n",
    "    train_dataset = CMIDataset(\n",
    "        table_df=train_df,\n",
    "        valid_ids=use_ids,\n",
    "        base_dir=\"../../inputs/series_train.parquet/\",\n",
    "        save_filename=nb_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[0][\"time_input\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "\n",
    "CV = []\n",
    "\n",
    "# use_ids = np.array(use_ids[:30]) # debug\n",
    "use_ids = np.array(use_ids)\n",
    "\n",
    "extract_df = train[train[\"id\"].isin(use_ids)].reset_index(drop=True)\n",
    "\n",
    "test_df = train[[\"id\", \"sii\"]].copy()\n",
    "# test_df[\"pred_sii\"] = 0\n",
    "oof_preds = []\n",
    "\n",
    "for fold, (train_ids, valid_ids) in enumerate(\n",
    "    skf.split(extract_df[\"id\"], extract_df[\"sii\"])\n",
    "):\n",
    "    print(f\"################### fold:{fold} ###################\")\n",
    "    best_valid_score = -100\n",
    "\n",
    "    train_ids = use_ids[train_ids]\n",
    "    valid_ids = use_ids[valid_ids]\n",
    "\n",
    "    train_dataset = CMIDataset(\n",
    "        table_df=train_df,\n",
    "        valid_ids=train_ids,\n",
    "        base_dir=\"../../inputs/series_train.parquet/\",\n",
    "        save_filename=nb_name,\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=30)\n",
    "\n",
    "    vlaid_dataset = CMIDataset(\n",
    "        table_df=train_df,\n",
    "        valid_ids=valid_ids,\n",
    "        base_dir=\"../../inputs/series_train.parquet/\",\n",
    "        save_filename=nb_name,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        vlaid_dataset, batch_size=1, shuffle=False, num_workers=30\n",
    "    )\n",
    "    # data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # model = TimeEncoder(input_size=26, hidden_size=13, num_layers=2).to(\"cuda\")\n",
    "    model = CMIModel(input_size=26, hidden_size=13, num_layers=2).to(\"cuda\")\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    spot_oof_preds = []\n",
    "\n",
    "    for epoch in range(5):\n",
    "        total_train_loss = 0\n",
    "        total_valid_loss = 0\n",
    "\n",
    "        train_pred = []\n",
    "        valid_pred = []\n",
    "        trian_gt = []\n",
    "        valid_gt = []\n",
    "\n",
    "        tq = tqdm(train_loader)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            table_input = data[\"table_input\"].to(\"cuda\")\n",
    "            time_input = data[\"time_input\"].to(\"cuda\")\n",
    "            mask = data[\"mask\"].to(\"cuda\").to(torch.float32)\n",
    "            target_ = data[\"output\"].to(\"cuda\")\n",
    "            optimizer.zero_grad()\n",
    "            output, attention_weight = model(table_input, time_input, active_mask=mask)\n",
    "            loss = criterion(output, target_)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            train_pred.append(output.detach().cpu().numpy())\n",
    "            trian_gt.append(target_.detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix(loss=total_train_loss / (i + 1))\n",
    "            tq.update()\n",
    "        tq.close()\n",
    "\n",
    "        tq = tqdm(valid_loader)\n",
    "        for i, data in enumerate(valid_loader):\n",
    "            table_input = data[\"table_input\"].to(\"cuda\")\n",
    "            time_input = data[\"time_input\"].to(\"cuda\")\n",
    "            mask = data[\"mask\"].to(\"cuda\").to(torch.float32)\n",
    "            target_ = data[\"output\"].to(\"cuda\")\n",
    "            output, attention_weight = model(table_input, time_input, active_mask=mask)\n",
    "            loss = criterion(output, target_)\n",
    "            total_valid_loss += loss.item()\n",
    "\n",
    "            valid_pred.append(output.detach().cpu().numpy())\n",
    "            valid_gt.append(target_.detach().cpu().numpy())\n",
    "\n",
    "            tq.set_postfix(loss=total_valid_loss / (i + 1))\n",
    "            tq.update()\n",
    "        tq.close()\n",
    "\n",
    "        metric_train_pred = np.concatenate(train_pred)\n",
    "        metric_valid_pred = np.concatenate(valid_pred)\n",
    "        metric_train_gt = np.concatenate(trian_gt)\n",
    "        metric_valid_gt = np.concatenate(valid_gt)\n",
    "\n",
    "        train_score = quadratic_weighted_kappa(\n",
    "            metric_train_gt, metric_train_pred.round(0).astype(int)\n",
    "        )\n",
    "\n",
    "        valid_score = quadratic_weighted_kappa(\n",
    "            metric_valid_gt, metric_valid_pred.round(0).astype(int)\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"epoch: {epoch}, loss: {total_train_loss / len(train_loader)}, valid_loss: {total_valid_loss / len(valid_loader)}, train_score: {train_score}, valid_score: {valid_score}\"\n",
    "        )\n",
    "\n",
    "        if valid_score > best_valid_score:\n",
    "            best_valid_score = valid_score\n",
    "            torch.save(model.state_dict(), f\"./assets/model_{fold}.pth\")\n",
    "\n",
    "            spot_oof_preds = []\n",
    "            for i, id_ in enumerate(valid_ids):\n",
    "                spot_oof_preds.append({\"id\": id_, \"pred_sii\": valid_pred[i][0][0]})\n",
    "\n",
    "    oof_preds.append(spot_oof_preds)\n",
    "    CV.append(best_valid_score)\n",
    "\n",
    "print(f\"CV: {np.mean(CV)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds_df = pd.concat([pd.DataFrame(p) for p in oof_preds], axis=0).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "oof_preds_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df = test_df.merge(oof_preds_df, on=\"id\", how=\"inner\")\n",
    "test_pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "KappaOPtimizer = minimize(\n",
    "    evaluate_predictions,\n",
    "    x0=[0.5, 1.5, 2.5],\n",
    "    args=(test_pred_df[\"sii\"], test_pred_df[\"pred_sii\"]),\n",
    "    method=\"Nelder-Mead\",\n",
    ")\n",
    "assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "\n",
    "oof_tuned = threshold_Rounder(test_pred_df[\"pred_sii\"], KappaOPtimizer.x)\n",
    "tKappa = quadratic_weighted_kappa(test_pred_df[\"sii\"], oof_tuned)\n",
    "print(f\"tuned Kappa: {tKappa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(KappaOPtimizer.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.deconv1 = nn.ConvTranspose1d(\n",
    "            32, 64, kernel_size=4, stride=2, padding=1\n",
    "        )  # 時間方向に拡大\n",
    "        self.deconv2 = nn.ConvTranspose1d(64, 32, kernel_size=4, stride=2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose1d(32, 15, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "        self.upsample = nn.Upsample(size=17280, mode=\"linear\")  # 最終的な長さに調整\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, days, time, channels = x.shape\n",
    "        x = x.view(batch * days, channels, time)  # (batch*days, 32, 10)\n",
    "\n",
    "        x = self.deconv1(x)\n",
    "        x = self.deconv2(x)\n",
    "        x = self.deconv3(x)\n",
    "\n",
    "        x = self.upsample(x)  # (batch*days, 15, 17280)\n",
    "        x = x.view(batch, days, 17280, 15)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# テスト\n",
    "encoder_output = torch.randn(1, 31, 10, 32)\n",
    "decoder = Decoder()\n",
    "output = decoder(encoder_output)\n",
    "print(output.shape)  # (1, 31, 17280, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
